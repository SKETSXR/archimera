{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3c3e674",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook will capture a workflow to use CLIP (vit-base-patch32) for extracting embeddings and FAISS for indexing these embeddings for searching similar sketches. This notebook will capture the embeddings from augmented sketches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d75374e",
   "metadata": {},
   "source": [
    "# 1. Without Preprocessing\n",
    "\n",
    "This block will include the workflow without any preprocessing involved. This workflow will include the following:\n",
    "1. The sketches will be converted from pdf to png formats.\n",
    "2. The embeddings will be extracted from these sketches.\n",
    "3. These embeddings will be stored and indexed in a faiss database.\n",
    "4. A report will be generated finding similar images for a query image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19209151",
   "metadata": {},
   "source": [
    "### 1.1. PDF to PNG Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b102bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "def convert_pdfs_to_pngs(input_folder, output_folder, dpi=600):\n",
    "    \"\"\"\n",
    "    Converts all single-page PDFs in a folder to PNGs with matching filenames.\n",
    "\n",
    "    ---\n",
    "    Parameters:\n",
    "        input_folder (str): Path to the folder containing pdf files.\n",
    "        output_folder (str): Path to the folder where PNGs will be saved.\n",
    "        dpi (int): Resolution for conversion. The default value is 600\n",
    "    \"\"\"\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Iterate through all PDF files in the input folder\n",
    "    for file_name in os.listdir(input_folder):\n",
    "        if file_name.lower().endswith(\".pdf\"):\n",
    "            pdf_path = os.path.join(input_folder, file_name)\n",
    "            base_name = os.path.splitext(file_name)[0]\n",
    "            output_path = os.path.join(output_folder, f\"{base_name}.png\")\n",
    "\n",
    "            # Convert PDF to image\n",
    "            images = convert_from_path(pdf_path, dpi=dpi)\n",
    "            img = images[0]\n",
    "\n",
    "            # Save as PNG\n",
    "            img.save(output_path, \"PNG\")\n",
    "            print(f\"Converted: {file_name} -> {base_name}.png\")\n",
    "    \n",
    "    print(\"\\n Conversion complete!\")\n",
    "\n",
    "# ** ADD THE FOLDER PATH TO PDF SKETCHES AND THE FOLDER PATH WHERE PNG SKETCHES ARE TO BE SAVED.\n",
    "input_folder = \"/home/ayushkum/archimera/inputs/input_pdf\"\n",
    "output_folder = \"/home/ayushkum/archimera/inputs/input_png\"\n",
    "dpi = 600 # ** DEFAULT VALUE IS 600, AS IT RESULTS IN BETTER RENDERING, CAN BE CHANGED TO HIGHER OR LOWER (NOT RECOMMENDED)\n",
    "\n",
    "# ! FUNCTION CALL\n",
    "convert_pdfs_to_pngs(\n",
    "    input_folder=input_folder,\n",
    "    output_folder=output_folder,\n",
    "    dpi=dpi\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a7664e",
   "metadata": {},
   "source": [
    "### 1.2. Extracting Embeddings and FAISS Storage\n",
    "\n",
    "The following code block will extract embeddings from sketches (assumes PNG) and build a FAISS Index, which would be indexed on `cosine-similarity`. This block would also create a mapping for later retrieval, and store it in a JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66edf7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 06:01:35.086773: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-30 06:01:35.448996: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-30 06:01:36.916708: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total embeddings computed: (88, 512)\n",
      "FAISS index saved at: /home/ayushkum/archimera/clip/augmented_sketch_index.faiss\n",
      "Mapping saved at: /home/ayushkum/archimera/clip/augmented_id_mapping.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import faiss\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "def build_faiss_index(\n",
    "        image_folder: str,\n",
    "        index_path: str = \"./sketch_index.faiss\",\n",
    "        mapping_path: str = \"./id_mapping.json\",\n",
    "        model_name: str = \"openai/clip-vit-base-patch32\",\n",
    "        distance_metric: str = \"L2\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Build a FAISS index from a folder of images using CLIP embeddings.\n",
    "\n",
    "    ---\n",
    "    Parameters:\n",
    "        image_folder (str): Path to the folder containing images.\n",
    "        index_path (str): Path where FAISS index file will be saved.\n",
    "        mapping_path (str): Path to save filename-to-index mapping JSON.\n",
    "        model_name (str): Pretrained CLIP model to use.\n",
    "        distance_metric (str): Distance metric to be used for computing similarity. Currently supports \"L2\" for Euclidean Distance, and \"cosine\" for cosine similarity.\n",
    "    \n",
    "    ---\n",
    "    Returns:\n",
    "        tuple (faiss.Index, dict): the faiss Index and filename mapping\n",
    "    \"\"\"\n",
    "    # Load model\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = CLIPModel.from_pretrained(model_name).to(device)\n",
    "    processor = CLIPProcessor.from_pretrained(model_name)\n",
    "\n",
    "    embeddings = []\n",
    "    filenames = []\n",
    "\n",
    "    # Process each image\n",
    "    for fname in os.listdir(image_folder):\n",
    "        if fname.lower().endswith(('.png','.jpg', '.jpeg')):\n",
    "            path = os.path.join(image_folder, fname)\n",
    "            try:\n",
    "                image = Image.open(path).convert(\"RGB\")\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping {fname}: {e}\")\n",
    "                continue\n",
    "\n",
    "            inputs = processor(images=image, return_tensors='pt').to(device)\n",
    "            with torch.no_grad():\n",
    "                image_embeds = model.get_image_features(**inputs)\n",
    "            \n",
    "            # Normalize for cosine or L2 comparison\n",
    "            image_embeds = image_embeds / image_embeds.norm(p=2, dim=-1, keepdim=True)\n",
    "            emb = image_embeds[0].cpu().numpy().astype('float32')\n",
    "\n",
    "            embeddings.append(emb)\n",
    "            filenames.append(fname)\n",
    "    \n",
    "    if not embeddings:\n",
    "        raise ValueError(f\"No valid images found in {image_folder}\")\n",
    "    \n",
    "    embeddings = np.stack(embeddings, axis=0)\n",
    "    print(f\"Total embeddings computed: {embeddings.shape}\")\n",
    "\n",
    "    # Select distance metric\n",
    "    dim = embeddings.shape[1]\n",
    "    if distance_metric.lower() == \"cosine\":\n",
    "        index = faiss.IndexFlatIP(dim)\n",
    "    else:\n",
    "        index = faiss.IndexFlatL2(dim)\n",
    "    \n",
    "    index.add(embeddings)\n",
    "\n",
    "    # Save index and mapping\n",
    "    faiss.write_index(index, index_path)\n",
    "    print(f\"FAISS index saved at: {index_path}\")\n",
    "\n",
    "    id_mapping = {i: filenames[i] for i in range(len(filenames))}\n",
    "    with open(mapping_path, \"w\") as f:\n",
    "        json.dump(id_mapping, f, indent=2)\n",
    "    print(f\"Mapping saved at: {mapping_path}\")\n",
    "\n",
    "    return index, id_mapping\n",
    "\n",
    "\n",
    "# ** ADD THE INPUT FOLDER CONTAINING SKETCH IMAGES\n",
    "image_folder = \"/home/ayushkum/archimera/augmented/input_png\"\n",
    "index_path = \"/home/ayushkum/archimera/clip/augmented_sketch_index.faiss\" # ** ADD PATH WHERE YOU WANT TO SAVE THE FAISS INDEX FILE.\n",
    "mapping_path = \"/home/ayushkum/archimera/clip/augmented_id_mapping.json\" # ** ADD PATH WHERE YOU WANT TO SAVE MAPPING BETWEEN FAISS ID AND FILENAME FOR LATER RETRIEVAL.\n",
    "model_name = \"openai/clip-vit-base-patch32\" # ** NAME OF THE MODEL TO BE USED FOR EMBEDDING EXTRACTION. CAN BE CHANGED BASED ON NEED.\n",
    "distance_metric = \"cosine\" # ** METRIC TO BE USED FOR INDEXING AND FINDING SIMILARITY. CURRENTLY RECOMMENDED TO USE COSINE.\n",
    "\n",
    "# ! FUNCTION CALL\n",
    "index, mapping = build_faiss_index(\n",
    "    image_folder=image_folder,\n",
    "    index_path=index_path,\n",
    "    mapping_path=mapping_path,\n",
    "    model_name=model_name,\n",
    "    distance_metric=distance_metric\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cdb334",
   "metadata": {},
   "source": [
    "### 1.3. Running a Query Image for Similarity Search\n",
    "\n",
    "The following code block can work with both pdf sketches and image sketches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab0c345",
   "metadata": {},
   "source": [
    "> RUN THE FOLLOWING BLOCK ONLY IF YOU HAVE PDF SKETCHES, OTHERWISE SKIP TO NEXT BLOCK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3191cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ! DO NOT RUN IF ALREADY HAVING QUERY SKETCHES IN PNG FORMAT\n",
    "query_pdf_path = \"/home/ayushkum/archimera/query_pdf\" # ** PATH TO FOLDER WHERE THE QUERY SKETCHES PDFs ARE STORED.\n",
    "query_png_path = \"/home/ayushkum/archimera/query_png\" # ** PATH TO FOLDER WHERE YOU WANT TO SAVE QUERY SKETCHES IN PNG FORM.\n",
    "\n",
    "# ! FUNCTION CALL\n",
    "convert_pdfs_to_pngs(\n",
    "    input_folder=query_pdf_path,\n",
    "    output_folder=query_png_path,\n",
    "    dpi=dpi\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae74ef24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Top similar sketches for pdf3_SIM: \n",
      "1. pdf7_aug7.png - score: 93.41%\n",
      "2. pdf5_aug6.png - score: 93.34%\n",
      "3. pdf5_aug3.png - score: 92.84%\n",
      "4. pdf8_aug8.png - score: 92.69%\n",
      "5. pdf5_aug1.png - score: 92.6%\n",
      "6. pdf5_aug8.png - score: 92.37%\n",
      "7. pdf5_aug5.png - score: 92.22%\n",
      "8. pdf7.png - score: 92.14%\n",
      "9. pdf8_aug10.png - score: 91.98%\n",
      "10. pdf7_aug3.png - score: 91.71%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import faiss\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "def search_similar_sketches(\n",
    "        query_path: str,\n",
    "        index_path: str = \"./sketch_index.faiss\",\n",
    "        mapping_path: str = \"./id_mapping.json\",\n",
    "        model_name: str = \"openai/clip-vit-base-patch32\",\n",
    "        top_k: int = 5,\n",
    "        distance_metric: str = \"L2\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Search for similar sketches using CLIP embeddings and a prebuilt FAISS index.\n",
    "\n",
    "    ---\n",
    "    Parameters:\n",
    "        query_path (str): Path to the query image (sketch).\n",
    "        index_path (str): Path to FAISS index file.\n",
    "        mapping_path (str): Path to JSON file containing ID -> filename mapping.\n",
    "        model_name (str): Pretrained CLIP model to use.\n",
    "        top_k (int): Number of most similar images to retrieve.\n",
    "        distance_metric (str): 'L2' or 'cosine' for similarity computation.\n",
    "    \n",
    "    ---\n",
    "    Returns:\n",
    "        list[dict]: Each item contains:\n",
    "        {\n",
    "            \"rank\": int,\n",
    "            \"filename\": str,\n",
    "            \"score\": float\n",
    "        }\n",
    "    \"\"\"\n",
    "\n",
    "    # Load FAISS index and mapping\n",
    "    index = faiss.read_index(index_path)\n",
    "    with open(mapping_path, \"r\") as f:\n",
    "        id_mapping = json.load(f)\n",
    "    \n",
    "    # Load CLIP Model\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = CLIPModel.from_pretrained(model_name).to(device)\n",
    "    processor = CLIPProcessor.from_pretrained(model_name)\n",
    "\n",
    "    # Compute query embedding\n",
    "    image = Image.open(query_path).convert(\"RGB\")\n",
    "    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        query_emb = model.get_image_features(**inputs)\n",
    "    query_emb = query_emb / query_emb.norm(p=2, dim=-1, keepdim=True)\n",
    "    query_emb = query_emb.cpu().numpy().astype('float32')\n",
    "\n",
    "    # Search top-k similar images\n",
    "    D, I = index.search(query_emb, top_k)\n",
    "\n",
    "    # Prepare results\n",
    "    results = []\n",
    "    for rank, (idx, dist) in enumerate(zip(I[0], D[0]), start=1):\n",
    "        fname = id_mapping.get(str(idx)) or id_mapping.get(idx)\n",
    "        # Convert distance to similarity if cosine metric is used\n",
    "        score = dist if distance_metric.lower() == \"cosine\" else (1 / (1 + dist))\n",
    "        results.append(\n",
    "            {\n",
    "                \"rank\": rank,\n",
    "                \"filename\": fname,\n",
    "                \"score\": float(score),\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    return results\n",
    "\n",
    "# ** ADD THE PATH TO FOLDER CONTAINING QUERY SKETCHES PNG\n",
    "query_sketch_path = \"/home/ayushkum/archimera/query_png\"\n",
    "index_path = \"/home/ayushkum/archimera/clip/augmented_sketch_index.faiss\" # ** ADD PATH TO THE FAISS INDEX FILE.\n",
    "mapping_path = \"/home/ayushkum/archimera/clip/augmented_id_mapping.json\" # ** ADD PATH TO ID MAPPING JSON FILE TO GET SIMILAR FILENAMES.\n",
    "top_k = 10 # ** NUMBER OF SIMILAR SKETCHES REQUIRED\n",
    "distance_metric = \"cosine\" # ** THE DISTANCE METRIC TO BE USED FOR COMPUTING SIMILARITY. DO USE SIMILAR METRIC WHICH WAS USED TO CREATE INDEX FILE.\n",
    "\n",
    "for filename in os.listdir(query_sketch_path):\n",
    "    if filename.lower().endswith(\".png\"):\n",
    "        query_image_path = os.path.join(query_sketch_path, filename)\n",
    "        base_name = os.path.splitext(filename)[0]\n",
    "        # ! FUNCTION CALL\n",
    "        results = search_similar_sketches(\n",
    "            query_path=query_image_path,\n",
    "            index_path=index_path,\n",
    "            mapping_path=mapping_path,\n",
    "            top_k=top_k,\n",
    "            distance_metric=distance_metric\n",
    "        )\n",
    "        print(f\"\\n Top similar sketches for {base_name}: \")\n",
    "        for r in results:\n",
    "            print(f\"{r['rank']}. {r['filename']} - score: {round(r['score'] * 100, 2)}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f829633",
   "metadata": {},
   "source": [
    "# 2. With Preprocessing\n",
    "\n",
    "This block will include the workflow with preprocessing. This workflow will include the following:\n",
    "1. The sketches will be converted from pdf to png formats. (CURRENTLY SKIPPING AS DONE IN `1.1`).\n",
    "2. The sketches will be preprocessed.\n",
    "3. The embeddings will be extracted from these sketches.\n",
    "4. These embeddings will be stored and indexed in a faiss database.\n",
    "5. A report will be generated finding similar images for a query image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcfa22c",
   "metadata": {},
   "source": [
    "### 2.1. Preprocessing Sketches\n",
    "\n",
    "The following code block will preprocess sketches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80df00e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 88 image(s) from '/home/ayushkum/archimera/augmented/input_png' → '/home/ayushkum/archimera/augmented/preprocessed_png'\n",
      "✅ Saved: /home/ayushkum/archimera/augmented/preprocessed_png/pdf7_aug3.png\n",
      "✅ Saved: /home/ayushkum/archimera/augmented/preprocessed_png/pdf8_aug6.png\n",
      "✅ Saved: /home/ayushkum/archimera/augmented/preprocessed_png/pdf5_aug2.png\n",
      "✅ Saved: /home/ayushkum/archimera/augmented/preprocessed_png/pdf2_aug5.png\n",
      "✅ Saved: /home/ayushkum/archimera/augmented/preprocessed_png/pdf6_aug6.png\n",
      "✅ Saved: /home/ayushkum/archimera/augmented/preprocessed_png/pdf6_aug9.png\n",
      "✅ Saved: /home/ayushkum/archimera/augmented/preprocessed_png/pdf4_aug1.png\n",
      "✅ Saved: /home/ayushkum/archimera/augmented/preprocessed_png/pdf3_aug8.png\n",
      "✅ Saved: /home/ayushkum/archimera/augmented/preprocessed_png/pdf6_aug8.png\n",
      "✅ Saved: /home/ayushkum/archimera/augmented/preprocessed_png/pdf1_aug5.png\n",
      "✅ Saved: /home/ayushkum/archimera/augmented/preprocessed_png/pdf3_aug9.png\n",
      "✅ Saved: /home/ayushkum/archimera/augmented/preprocessed_png/pdf2_aug6.png\n",
      "✅ Saved: /home/ayushkum/archimera/augmented/preprocessed_png/pdf4_aug4.png\n",
      "✅ Saved: /home/ayushkum/archimera/augmented/preprocessed_png/pdf5.png\n",
      "✅ Saved: /home/ayushkum/archimera/augmented/preprocessed_png/pdf2_aug8.png\n",
      "✅ Saved: /home/ayushkum/archimera/augmented/preprocessed_png/pdf8.png\n",
      "✅ Saved: /home/ayushkum/archimera/augmented/preprocessed_png/pdf4_aug5.png\n",
      "✅ Saved: /home/ayushkum/archimera/augmented/preprocessed_png/pdf4.png\n",
      "✅ Saved: /home/ayushkum/archimera/augmented/preprocessed_png/pdf2_aug7.png\n",
      "✅ Saved: /home/ayushkum/archimera/augmented/preprocessed_png/pdf1_aug3.png\n",
      "✅ Saved: /home/ayushkum/archimera/augmented/preprocessed_png/pdf3.png\n",
      "✅ Saved: /home/ayushkum/archimera/augmented/preprocessed_png/pdf5_aug10.png\n",
      "✅ Saved: /home/ayushkum/archimera/augmented/preprocessed_png/pdf4_aug9.png\n",
      "✅ Saved: /home/ayushkum/archimera/augmented/preprocessed_png/pdf7_aug2.png\n",
      "✅ Saved: /home/ayushkum/archimera/augmented/preprocessed_png/pdf4_aug2.png\n",
      "✅ Saved: /home/ayushkum/archimera/augmented/preprocessed_png/pdf5_aug7.png\n",
      "✅ Saved: /home/ayushkum/archimera/augmented/preprocessed_png/pdf3_aug10.png\n",
      "✅ Saved: /home/ayushkum/archimera/augmented/preprocessed_png/pdf5_aug4.png\n",
      "✅ Saved: /home/ayushkum/archimera/augmented/preprocessed_png/pdf1_aug2.png\n",
      "✅ Saved: /home/ayushkum/archimera/augmented/preprocessed_png/pdf2_aug1.png\n",
      "✅ Saved: /home/ayushkum/archimera/augmented/preprocessed_png/pdf7_aug9.png\n",
      "✅ Saved: /home/ayushkum/archimera/augmented/preprocessed_png/pdf8_aug2.png\n",
      "✅ Saved: /home/ayushkum/archimera/augmented/preprocessed_png/pdf3_aug6.png\n",
      "✅ Saved: /home/ayushkum/archimera/augmented/preprocessed_png/pdf2_aug3.png\n",
      "✅ Saved: /home/ayushkum/archimera/augmented/preprocessed_png/pdf8_aug7.png\n",
      "✅ Saved: /home/ayushkum/archimera/augmented/preprocessed_png/pdf1_aug7.png\n",
      "✅ Saved: /home/ayushkum/archimera/augmented/preprocessed_png/pdf1_aug4.png\n",
      "✅ Saved: /home/ayushkum/archimera/augmented/preprocessed_png/pdf3_aug2.png\n",
      "✅ Saved: /home/ayushkum/archimera/augmented/preprocessed_png/pdf2_aug2.png\n",
      "✅ Saved: /home/ayushkum/archimera/augmented/preprocessed_png/pdf1_aug8.png\n",
      "✅ Saved: /home/ayushkum/archimera/augmented/preprocessed_png/pdf8_aug10.png\n",
      "✅ Saved: /home/ayushkum/archimera/augmented/preprocessed_png/pdf6_aug7.png\n",
      "✅ Saved: /home/ayushkum/archimera/augmented/preprocessed_png/pdf8_aug1.png\n",
      "✅ Saved: /home/ayushkum/archimera/augmented/preprocessed_png/pdf1_aug1.png\n",
      "✅ Saved: /home/ayushkum/archimera/augmented/preprocessed_png/pdf5_aug9.png\n",
      "✅ Saved: /home/ayushkum/archimera/augmented/preprocessed_png/pdf1_aug6.png\n",
      "✅ Saved: /home/ayushkum/archimera/augmented/preprocessed_png/pdf7_aug10.png\n",
      "✅ Saved: /home/ayushkum/archimera/augmented/preprocessed_png/pdf3_aug5.png\n",
      "✅ Saved: /home/ayushkum/archimera/augmented/preprocessed_png/pdf1.png\n",
      "✅ Saved: /home/ayushkum/archimera/augmented/preprocessed_png/pdf7_aug7.png\n",
      "✅ Saved: /home/ayushkum/archimera/augmented/preprocessed_png/pdf7_aug6.png\n",
      "✅ Saved: /home/ayushkum/archimera/augmented/preprocessed_png/pdf6.png\n",
      "✅ Saved: /home/ayushkum/archimera/augmented/preprocessed_png/pdf4_aug7.png\n",
      "✅ Saved: /home/ayushkum/archimera/augmented/preprocessed_png/pdf5_aug6.png\n",
      "✅ Saved: /home/ayushkum/archimera/augmented/preprocessed_png/pdf1_aug9.png\n",
      "✅ Saved: /home/ayushkum/archimera/augmented/preprocessed_png/pdf8_aug9.png\n",
      "✅ Saved: /home/ayushkum/archimera/augmented/preprocessed_png/pdf5_aug8.png\n",
      "✅ Saved: /home/ayushkum/archimera/augmented/preprocessed_png/pdf2_aug10.png\n",
      "✅ Saved: /home/ayushkum/archimera/augmented/preprocessed_png/pdf4_aug6.png\n",
      "✅ Saved: /home/ayushkum/archimera/augmented/preprocessed_png/pdf5_aug5.png\n",
      "✅ Saved: /home/ayushkum/archimera/augmented/preprocessed_png/pdf6_aug10.png\n",
      "✅ Saved: /home/ayushkum/archimera/augmented/preprocessed_png/pdf6_aug1.png\n",
      "✅ Saved: /home/ayushkum/archimera/augmented/preprocessed_png/pdf3_aug4.png\n",
      "✅ Saved: /home/ayushkum/archimera/augmented/preprocessed_png/pdf6_aug4.png\n",
      "✅ Saved: /home/ayushkum/archimera/augmented/preprocessed_png/pdf4_aug3.png\n",
      "✅ Saved: /home/ayushkum/archimera/augmented/preprocessed_png/pdf7_aug4.png\n",
      "✅ Saved: /home/ayushkum/archimera/augmented/preprocessed_png/pdf6_aug2.png\n",
      "✅ Saved: /home/ayushkum/archimera/augmented/preprocessed_png/pdf8_aug3.png\n",
      "✅ Saved: /home/ayushkum/archimera/augmented/preprocessed_png/pdf1_aug10.png\n",
      "✅ Saved: /home/ayushkum/archimera/augmented/preprocessed_png/pdf3_aug7.png\n",
      "✅ Saved: /home/ayushkum/archimera/augmented/preprocessed_png/pdf7.png\n",
      "✅ Saved: /home/ayushkum/archimera/augmented/preprocessed_png/pdf2_aug9.png\n",
      "✅ Saved: /home/ayushkum/archimera/augmented/preprocessed_png/pdf5_aug3.png\n",
      "✅ Saved: /home/ayushkum/archimera/augmented/preprocessed_png/pdf8_aug4.png\n",
      "✅ Saved: /home/ayushkum/archimera/augmented/preprocessed_png/pdf7_aug8.png\n",
      "✅ Saved: /home/ayushkum/archimera/augmented/preprocessed_png/pdf6_aug5.png\n",
      "✅ Saved: /home/ayushkum/archimera/augmented/preprocessed_png/pdf8_aug5.png\n",
      "✅ Saved: /home/ayushkum/archimera/augmented/preprocessed_png/pdf5_aug1.png\n",
      "✅ Saved: /home/ayushkum/archimera/augmented/preprocessed_png/pdf7_aug5.png\n",
      "✅ Saved: /home/ayushkum/archimera/augmented/preprocessed_png/pdf6_aug3.png\n",
      "✅ Saved: /home/ayushkum/archimera/augmented/preprocessed_png/pdf3_aug3.png\n",
      "✅ Saved: /home/ayushkum/archimera/augmented/preprocessed_png/pdf4_aug8.png\n",
      "✅ Saved: /home/ayushkum/archimera/augmented/preprocessed_png/pdf7_aug1.png\n",
      "✅ Saved: /home/ayushkum/archimera/augmented/preprocessed_png/pdf8_aug8.png\n",
      "✅ Saved: /home/ayushkum/archimera/augmented/preprocessed_png/pdf3_aug1.png\n",
      "✅ Saved: /home/ayushkum/archimera/augmented/preprocessed_png/pdf4_aug10.png\n",
      "✅ Saved: /home/ayushkum/archimera/augmented/preprocessed_png/pdf2_aug4.png\n",
      "✅ Saved: /home/ayushkum/archimera/augmented/preprocessed_png/pdf2.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_sketch(img_path):\n",
    "    import cv2, numpy as np\n",
    "\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        raise ValueError(f\"Could not read {img_path}\")\n",
    "\n",
    "    # Step 1: Optional small padding to preserve edges after augmentations\n",
    "    img = cv2.copyMakeBorder(img, 5, 5, 5, 5, cv2.BORDER_REFLECT)\n",
    "\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Step 2: Conditional smoothing (avoid oversmoothing augmented images)\n",
    "    blur = cv2.bilateralFilter(gray, 9, 75, 75) if np.std(gray) > 20 else gray\n",
    "\n",
    "    norm = cv2.normalize(blur, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    edges = cv2.Canny(norm, 30, 100)\n",
    "    edges = cv2.dilate(edges, np.ones((2, 2), np.uint8), iterations=1)\n",
    "\n",
    "    binary = cv2.adaptiveThreshold(\n",
    "        edges, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2\n",
    "    )\n",
    "\n",
    "    kernel = np.ones((3, 3), np.uint8)\n",
    "    refined = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)\n",
    "    # processed = cv2.bitwise_not(refined)\n",
    "\n",
    "    processed_rgb = cv2.cvtColor(refined, cv2.COLOR_GRAY2RGB)#(processed, cv2.COLOR_GRAY2RGB)\n",
    "    return processed_rgb\n",
    "\n",
    "\n",
    "def batch_preprocess_sketches(input_folder, output_folder):\n",
    "    \"\"\"\n",
    "    Applies `preprocess_sketch()` to all PNG images in a given folder\n",
    "    and saves them into an output folder with identical filenames.\n",
    "\n",
    "    Args:\n",
    "        input_folder (str): Path to the directory containing input PNGs.\n",
    "        output_folder (str): Path to save the processed PNGs.\n",
    "\n",
    "    Example:\n",
    "        >>> batch_preprocess_sketches(\"input_png\", \"output_png\")\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    image_files = [\n",
    "        f for f in os.listdir(input_folder)\n",
    "        if f.lower().endswith(\".png\")\n",
    "    ]\n",
    "\n",
    "    if not image_files:\n",
    "        print(f\"No PNG images found in '{input_folder}'.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Processing {len(image_files)} image(s) from '{input_folder}' → '{output_folder}'\")\n",
    "\n",
    "    for filename in image_files:\n",
    "        input_path = os.path.join(input_folder, filename)\n",
    "        output_path = os.path.join(output_folder, filename)\n",
    "\n",
    "        try:\n",
    "            processed_img = preprocess_sketch(input_path)\n",
    "            cv2.imwrite(output_path, processed_img)\n",
    "            print(f\"✅ Saved: {output_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing {filename}: {e}\")\n",
    "\n",
    "\n",
    "# ** ADD PATH TO FOLDER CONTAINING INPUT PNG SKETCHES\n",
    "input_path = \"/home/ayushkum/archimera/augmented/input_png\"\n",
    "output_path = \"/home/ayushkum/archimera/augmented/preprocessed_png\" # ** ADD PATH TO FOLDER WHERE YOU WANT TO SAVE PREPROCESSED SKETCHES\n",
    "\n",
    "# ! FUNCTION CALL\n",
    "batch_preprocess_sketches(input_folder=input_path, output_folder=output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb1edb7",
   "metadata": {},
   "source": [
    "### 2.2. Extracting Embeddings and FAISS storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5202892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total embeddings computed: (88, 512)\n",
      "FAISS index saved at: /home/ayushkum/archimera/clip/augmented_preprocessed_sketch_index.faiss\n",
      "Mapping saved at: /home/ayushkum/archimera/clip/augmented_preprocessed_id_mapping.json\n"
     ]
    }
   ],
   "source": [
    "# ** ADD THE INPUT FOLDER CONTAINING SKETCH IMAGES\n",
    "image_folder = \"/home/ayushkum/archimera/augmented/preprocessed_png\"\n",
    "index_path = \"/home/ayushkum/archimera/clip/augmented_preprocessed_sketch_index.faiss\" # ** ADD PATH WHERE YOU WANT TO SAVE THE FAISS INDEX FILE.\n",
    "mapping_path = \"/home/ayushkum/archimera/clip/augmented_preprocessed_id_mapping.json\" # ** ADD PATH WHERE YOU WANT TO SAVE MAPPING BETWEEN FAISS ID AND FILENAME FOR LATER RETRIEVAL.\n",
    "model_name = \"openai/clip-vit-base-patch32\" # ** NAME OF THE MODEL TO BE USED FOR EMBEDDING EXTRACTION. CAN BE CHANGED BASED ON NEED.\n",
    "distance_metric = \"cosine\" # ** METRIC TO BE USED FOR INDEXING AND FINDING SIMILARITY. CURRENTLY RECOMMENDED TO USE COSINE.\n",
    "\n",
    "# ! FUNCTION CALL\n",
    "index, mapping = build_faiss_index(\n",
    "    image_folder=image_folder,\n",
    "    index_path=index_path,\n",
    "    mapping_path=mapping_path,\n",
    "    model_name=model_name,\n",
    "    distance_metric=distance_metric\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2be6aeb",
   "metadata": {},
   "source": [
    "### 2.3 Running a Query Sketch for Similarity Search\n",
    "\n",
    "We would have to first preprocess the query sketch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a39ab18b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1 image(s) from '/home/ayushkum/archimera/query_png' → '/home/ayushkum/archimera/preprocessed_query_png'\n",
      "✅ Saved: /home/ayushkum/archimera/preprocessed_query_png/pdf3_SIM.png\n"
     ]
    }
   ],
   "source": [
    "# ** ADD PATH TO FOLDER CONTAINING QUERY PNG SKETCHES\n",
    "input_path = \"/home/ayushkum/archimera/query_png\"\n",
    "output_path = \"/home/ayushkum/archimera/preprocessed_query_png\" # ** ADD PATH TO FOLDER WHERE YOU WANT TO SAVE PREPROCESSED SKETCHES\n",
    "\n",
    "# ! FUNCTION CALL\n",
    "batch_preprocess_sketches(input_folder=input_path, output_folder=output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "866c6d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Top similar sketches for pdf3_SIM: \n",
      "1. pdf4.png - score: 94.31%\n",
      "2. pdf4_aug3.png - score: 94.16%\n",
      "3. pdf4_aug7.png - score: 93.06%\n",
      "4. pdf4_aug10.png - score: 92.75%\n",
      "5. pdf4_aug2.png - score: 92.4%\n",
      "6. pdf4_aug9.png - score: 91.56%\n",
      "7. pdf3_aug5.png - score: 91.43%\n",
      "8. pdf5_aug10.png - score: 91.4%\n",
      "9. pdf3.png - score: 91.36%\n",
      "10. pdf3_aug4.png - score: 91.27%\n"
     ]
    }
   ],
   "source": [
    "# ** ADD THE PATH TO FOLDER CONTAINING QUERY PREPROCESSED SKETCHES PNG\n",
    "query_sketch_path = \"/home/ayushkum/archimera/preprocessed_query_png\"\n",
    "index_path = \"/home/ayushkum/archimera/clip/augmented_preprocessed_sketch_index.faiss\" # ** ADD PATH TO THE FAISS INDEX FILE.\n",
    "mapping_path = \"/home/ayushkum/archimera/clip/augmented_preprocessed_id_mapping.json\" # ** ADD PATH TO ID MAPPING JSON FILE TO GET SIMILAR FILENAMES.\n",
    "top_k = 10 # ** NUMBER OF SIMILAR SKETCHES REQUIRED\n",
    "distance_metric = \"cosine\" # ** THE DISTANCE METRIC TO BE USED FOR COMPUTING SIMILARITY. DO USE SIMILAR METRIC WHICH WAS USED TO CREATE INDEX FILE.\n",
    "\n",
    "for filename in os.listdir(query_sketch_path):\n",
    "    if filename.lower().endswith(\".png\"):\n",
    "        query_image_path = os.path.join(query_sketch_path, filename)\n",
    "        base_name = os.path.splitext(filename)[0]\n",
    "        # ! FUNCTION CALL\n",
    "        results = search_similar_sketches(\n",
    "            query_path=query_image_path,\n",
    "            index_path=index_path,\n",
    "            mapping_path=mapping_path,\n",
    "            top_k=top_k,\n",
    "            distance_metric=distance_metric\n",
    "        )\n",
    "        print(f\"\\n Top similar sketches for {base_name}: \")\n",
    "        for r in results:\n",
    "            print(f\"{r['rank']}. {r['filename']} - score: {round(r['score'] * 100, 2)}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
