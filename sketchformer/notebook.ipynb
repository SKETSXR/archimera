{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4ec871a",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook will implement the workflow with SketchFormer as the image embedding extraction model. It will contain results for both original and augmented datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682af91e",
   "metadata": {},
   "source": [
    "# Original Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee1e899a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Converted pdf1.png | Length: 200\n",
      "âœ… Converted pdf2.png | Length: 200\n",
      "âœ… Converted pdf3.png | Length: 200\n",
      "âœ… Converted pdf4.png | Length: 200\n",
      "âœ… Converted pdf5.png | Length: 200\n",
      "âœ… Converted pdf6.png | Length: 200\n",
      "âœ… Converted pdf7.png | Length: 200\n",
      "âœ… Converted pdf8.png | Length: 200\n",
      "\n",
      "ðŸ’¾ Saved dataset: /home/ayushkum/archimera/sketchformer/sketchformer_dataset/chunk_0.npz\n",
      "   Shape: (8, 200, 5)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# pngs_to_continuous_strokes.py\n",
    "\"\"\"\n",
    "Convert PNG sketch images into continuous stroke sequences expected by SketchFormer.\n",
    "Each stroke vector has 5 values: [dx, dy, pen_down, pen_up, pen_end/pad].\n",
    "Sequences longer than `max_seq_len` are adaptively reduced using average pooling\n",
    "(for continuous deltas) and voting-based aggregation (for binary pen states).\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from skimage.morphology import skeletonize\n",
    "from glob import glob\n",
    "\n",
    "# -------------------- Image Preprocessing --------------------\n",
    "def preprocess_image(path, target_size=None):\n",
    "    \"\"\"\n",
    "    Reads an image, binarizes, inverts (so stroke pixels=1), and skeletonizes.\n",
    "    \"\"\"\n",
    "    img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "    if img is None:\n",
    "        raise RuntimeError(f\"Could not read {path}\")\n",
    "    if target_size:\n",
    "        img = cv2.resize(img, target_size, interpolation=cv2.INTER_AREA)\n",
    "    # Adaptive threshold using Otsu\n",
    "    _, th = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    if np.mean(th) > 127:  # ensure black strokes on white bg\n",
    "        th = 255 - th\n",
    "    sk = skeletonize((th > 0))\n",
    "    return (sk.astype(np.uint8) * 255)\n",
    "\n",
    "\n",
    "# -------------------- Contour Extraction --------------------\n",
    "def image_to_polylines(img, min_length=5, approx_epsilon=2.0):\n",
    "    \"\"\"\n",
    "    Extracts skeleton contours and approximates them as polylines.\n",
    "    \"\"\"\n",
    "    contours, _ = cv2.findContours(img.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)\n",
    "    polylines = []\n",
    "    for cnt in contours:\n",
    "        if len(cnt) < min_length:\n",
    "            continue\n",
    "        cnt = cnt.squeeze()\n",
    "        if cnt.ndim != 2:\n",
    "            continue\n",
    "        approx = cv2.approxPolyDP(cnt.astype(np.int32), approx_epsilon, False)\n",
    "        approx = approx.squeeze()\n",
    "        if approx.ndim != 2:\n",
    "            continue\n",
    "        polylines.append(approx.astype(np.float32))\n",
    "    return polylines\n",
    "\n",
    "\n",
    "# -------------------- Polyline â†’ Sequence --------------------\n",
    "def polylines_to_continuous_seq(polylines):\n",
    "    \"\"\"\n",
    "    Convert polylines to [dx, dy, pen_down, pen_up, pen_end] sequence.\n",
    "    \"\"\"\n",
    "    seq = []\n",
    "    for i, poly in enumerate(polylines):\n",
    "        if poly.shape[0] < 2:\n",
    "            continue\n",
    "        deltas = np.diff(poly, axis=0)  # shape (n-1,2)\n",
    "        for dx, dy in deltas:\n",
    "            seq.append([float(dx), float(dy), 1.0, 0.0, 0.0])  # pen_down\n",
    "        # mark stroke end\n",
    "        if i < len(polylines) - 1:\n",
    "            seq.append([0.0, 0.0, 0.0, 1.0, 0.0])  # pen_up\n",
    "    if not seq:\n",
    "        seq = [[0.0, 0.0, 0.0, 0.0, 1.0]]  # fallback single token\n",
    "    return np.array(seq, dtype=np.float32)\n",
    "\n",
    "\n",
    "# -------------------- Sequence Pooling --------------------\n",
    "def pooled_pen_state(chunk, idx):\n",
    "    \"\"\"\n",
    "    Decide binary pen state (pen_down/up/end) for a pooled segment.\n",
    "    Uses hybrid logic: majority vote with fallback if any '1' present.\n",
    "    \"\"\"\n",
    "    avg = np.mean(chunk[:, idx])\n",
    "    if avg >= 0.5:\n",
    "        return 1.0\n",
    "    # elif np.any(chunk[:, idx] > 0.5):\n",
    "    #     return 1.0\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "def average_pool_sequence(seq, target_len):\n",
    "    \"\"\"\n",
    "    Average pool a (N,5) sequence to target_len for SketchFormer compatibility.\n",
    "    - dx, dy averaged normally\n",
    "    - pen_* flags aggregated via hybrid voting\n",
    "    \"\"\"\n",
    "    n = len(seq)\n",
    "    if n <= target_len:\n",
    "        return seq  # no pooling needed\n",
    "\n",
    "    step = n / target_len\n",
    "    pooled = []\n",
    "    for i in range(target_len):\n",
    "        start = int(i * step)\n",
    "        end = int((i + 1) * step)\n",
    "        chunk = seq[start:end]\n",
    "        if len(chunk) == 0:\n",
    "            continue\n",
    "        dx, dy = np.mean(chunk[:, 0]), np.mean(chunk[:, 1])\n",
    "        pen_down = pooled_pen_state(chunk, 2)\n",
    "        pen_up = pooled_pen_state(chunk, 3)\n",
    "        pen_end = pooled_pen_state(chunk, 4)\n",
    "        pooled.append([dx, dy, pen_down, pen_up, pen_end])\n",
    "\n",
    "    return np.array(pooled, dtype=np.float32)\n",
    "\n",
    "\n",
    "# -------------------- Sequence Padding --------------------\n",
    "def pad_sequence(seq, max_seq_len):\n",
    "    \"\"\"\n",
    "    Pad sequence to fixed length using [0,0,0,0,1].\n",
    "    \"\"\"\n",
    "    pad_token = [0.0, 0.0, 0.0, 0.0, 1.0]\n",
    "    if len(seq) > max_seq_len:\n",
    "        seq = seq[:max_seq_len]\n",
    "    elif len(seq) < max_seq_len:\n",
    "        pad = np.tile(pad_token, (max_seq_len - len(seq), 1))\n",
    "        seq = np.vstack([seq, pad])\n",
    "    return seq\n",
    "\n",
    "\n",
    "# -------------------- Conversion Pipeline --------------------\n",
    "def img_path_to_seq(path, target_size=None, max_seq_len=200):\n",
    "    \"\"\"\n",
    "    Complete conversion from PNG â†’ skeleton â†’ polylines â†’ sequence (pooled + padded).\n",
    "    \"\"\"\n",
    "    sk = preprocess_image(path, target_size=target_size)\n",
    "    polylines = image_to_polylines(sk, min_length=8, approx_epsilon=2.0)\n",
    "    seq = polylines_to_continuous_seq(polylines)\n",
    "\n",
    "    # adaptive pooling if too long\n",
    "    if len(seq) > max_seq_len:\n",
    "        seq = average_pool_sequence(seq, max_seq_len)\n",
    "\n",
    "    seq = pad_sequence(seq, max_seq_len)\n",
    "    return seq\n",
    "\n",
    "\n",
    "# -------------------- Main Routine --------------------\n",
    "def main(png_folder, out_npz, target_size=None, max_seq_len=200):\n",
    "    paths = sorted(glob(os.path.join(png_folder, '*.png')))\n",
    "    X, filenames = [], []\n",
    "\n",
    "    for p in paths:\n",
    "        try:\n",
    "            seq = img_path_to_seq(p, target_size=target_size, max_seq_len=max_seq_len)\n",
    "            X.append(seq)\n",
    "            filenames.append(os.path.basename(p))\n",
    "            print(f\"âœ… Converted {os.path.basename(p)} | Length: {np.sum(seq[..., -1] != 1)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Failed {p}: {e}\")\n",
    "\n",
    "    if not X:\n",
    "        raise RuntimeError(\"No sequences produced.\")\n",
    "\n",
    "    X = np.stack(X, axis=0)  # (N, max_seq_len, 5)\n",
    "    np.savez(out_npz, x=X, filenames=np.array(filenames))\n",
    "    print(f\"\\nðŸ’¾ Saved dataset: {out_npz}\")\n",
    "    print(f\"   Shape: {X.shape}\")\n",
    "\n",
    "\n",
    "# -------------------- Driver Code --------------------\n",
    "# if __name__ == \"__main__\":\n",
    "png_folder = \"/home/ayushkum/archimera/inputs/input_png\"\n",
    "out_npz = \"/home/ayushkum/archimera/sketchformer/sketchformer_dataset/chunk_0.npz\"\n",
    "main(png_folder=png_folder, out_npz=out_npz)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7af39fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 11:54:06.867977: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-30 11:54:07.191757: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-30 11:54:08.556447: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1761825252.276403  108514 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13510 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Restored checkpoint: ./sketch-transformer-tf2-cvpr_tform_cont/weights/ckpt-12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ayushkum/archimera/env/lib/python3.12/site-packages/keras/src/layers/layer.py:970: UserWarning: Layer 'encoder_layer_4' (of type EncoderLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/ayushkum/archimera/env/lib/python3.12/site-packages/keras/src/layers/layer.py:970: UserWarning: Layer 'encoder_layer_5' (of type EncoderLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/ayushkum/archimera/env/lib/python3.12/site-packages/keras/src/layers/layer.py:970: UserWarning: Layer 'encoder_layer_6' (of type EncoderLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/ayushkum/archimera/env/lib/python3.12/site-packages/keras/src/layers/layer.py:970: UserWarning: Layer 'encoder_layer_7' (of type EncoderLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/ayushkum/archimera/env/lib/python3.12/site-packages/keras/src/layers/layer.py:970: UserWarning: Layer 'encoder_1' (of type Encoder) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/ayushkum/archimera/env/lib/python3.12/site-packages/keras/src/ops/nn.py:938: UserWarning: You are using a softmax over axis -1 of a tensor of shape (1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/8] Embedded pdf1.png -> (128,)\n",
      "[2/8] Embedded pdf2.png -> (128,)\n",
      "[3/8] Embedded pdf3.png -> (128,)\n",
      "[4/8] Embedded pdf4.png -> (128,)\n",
      "[5/8] Embedded pdf5.png -> (128,)\n",
      "[6/8] Embedded pdf6.png -> (128,)\n",
      "[7/8] Embedded pdf7.png -> (128,)\n",
      "[8/8] Embedded pdf8.png -> (128,)\n",
      "[INFO] Saved embeddings: ./sketchformer_embeddings.npy shape: (8, 128)\n",
      "[INFO] Saved mapping: ./id_mapping_sketchformer.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[ 1.010989  , -0.41237274,  0.17688197, ...,  0.66918933,\n",
       "          0.26845735, -0.75079656],\n",
       "        [ 0.68106216, -0.65177184, -0.5079062 , ..., -0.03892206,\n",
       "          0.48827237, -0.72250056],\n",
       "        [ 1.0475414 , -0.7934085 , -0.10133129, ...,  0.4485203 ,\n",
       "          0.09297235, -1.0622104 ],\n",
       "        ...,\n",
       "        [ 1.1594973 , -0.9593415 , -0.20265836, ...,  0.5945516 ,\n",
       "         -0.02203569, -0.8526877 ],\n",
       "        [ 0.4188169 , -0.82615554, -0.21041293, ...,  0.2429265 ,\n",
       "          1.0349033 , -0.3070404 ],\n",
       "        [ 0.8352951 , -1.0711412 , -0.29345888, ...,  0.4724831 ,\n",
       "         -0.2534674 , -1.072026  ]], shape=(8, 128), dtype=float32),\n",
       " {0: 'pdf1.png',\n",
       "  1: 'pdf2.png',\n",
       "  2: 'pdf3.png',\n",
       "  3: 'pdf4.png',\n",
       "  4: 'pdf5.png',\n",
       "  5: 'pdf6.png',\n",
       "  6: 'pdf7.png',\n",
       "  7: 'pdf8.png'})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# extract_sketchformer_embeddings.py\n",
    "\"\"\"\n",
    "Load SketchFormer model from sketchformer-master, restore checkpoint, and extract embeddings.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "sys.path.append(\"./sketchformer_master\")\n",
    "\n",
    "# make sure repo is on path\n",
    "from sketchformer_master.models.sketchformer import Transformer as SketchTransformer\n",
    "from sketchformer_master.utils.hparams import HParams\n",
    "\n",
    "class DummyDataset:\n",
    "    \"\"\"Minimal dataset shim required by model __init__\"\"\"\n",
    "    def __init__(self, max_seq_len=200):\n",
    "        self.hps = {'use_continuous_data': True, 'max_seq_len': max_seq_len}\n",
    "        self.n_samples = 1\n",
    "        self.num_classes = 1\n",
    "        self.n_classes = 1\n",
    "        self.tokenizer = None\n",
    "\n",
    "def load_hparams_from_config(config_path=None):\n",
    "    \"\"\"\n",
    "    Create hparams from default then override with config.json if present.\n",
    "    \"\"\"\n",
    "    hps = SketchTransformer.specific_default_hparams()\n",
    "    if config_path and os.path.exists(config_path):\n",
    "        import json\n",
    "        cfg = json.load(open(config_path))\n",
    "        for k, v in cfg.items():\n",
    "            # only update keys that exist; HParams supports item assignment\n",
    "            try:\n",
    "                hps[k] = v\n",
    "            except Exception:\n",
    "                # ignore unknown keys\n",
    "                pass\n",
    "    return hps\n",
    "\n",
    "def build_model_and_restore(weights_dir, max_seq_len=200, config_json=None):\n",
    "    dataset = DummyDataset(max_seq_len=max_seq_len)\n",
    "    hps = load_hparams_from_config(config_json)\n",
    "\n",
    "    \n",
    "    # instantiate model\n",
    "    model = SketchTransformer(hps, dataset, out_dir='.', experiment_id='inference')\n",
    "    # build model graph: call build_model to create layers\n",
    "    model.build_model()\n",
    "    # restore checkpoint\n",
    "    ckpt = tf.train.Checkpoint(model=model)\n",
    "    latest = tf.train.latest_checkpoint(weights_dir)\n",
    "    if latest is None:\n",
    "        raise RuntimeError(f\"No checkpoint found in {weights_dir}\")\n",
    "    ckpt.restore(latest).expect_partial()\n",
    "    print(\"[INFO] Restored checkpoint:\", latest)\n",
    "    return model\n",
    "\n",
    "def extract_embeddings_from_npz(npz_path, model, out_embeddings_path=\"sketchformer_embeddings.npy\", out_map_path=\"id_mapping.json\"):\n",
    "    d = np.load(npz_path, allow_pickle=True)\n",
    "    X = d['x']  # shape (N, seq_len, 5)\n",
    "    filenames = d.get('filenames', None)\n",
    "    if filenames is None:\n",
    "        # try fallback\n",
    "        filenames = [f\"img_{i}.png\" for i in range(X.shape[0])]\n",
    "    embeddings = []\n",
    "    for i in range(X.shape[0]):\n",
    "        seq = X[i]  # (seq_len, 5)\n",
    "        # model.encode_from_seq expects a 1D or 2D array representing the sequence (it casts to np.array)\n",
    "        seq = np.expand_dims(seq, axis=0)\n",
    "        try:\n",
    "            out = model.encode_from_seq(seq)\n",
    "            emb = out['embedding']  # should be (1, lowerdim) or (1, seq_len, ...)\n",
    "            # ensure emb is numpy\n",
    "            if hasattr(emb, 'numpy'):\n",
    "                emb_np = emb.numpy()\n",
    "            else:\n",
    "                emb_np = np.array(emb)\n",
    "            # if emb has shape (1, d) or (1, seq_len, d), reduce to (d,)\n",
    "            if emb_np.ndim == 3:\n",
    "                # average across sequence axis if needed\n",
    "                emb_np = emb_np.mean(axis=1)\n",
    "            emb_np = emb_np.reshape(-1)\n",
    "            embeddings.append(emb_np.astype('float32'))\n",
    "            print(f\"[{i+1}/{X.shape[0]}] Embedded {filenames[i]} -> {emb_np.shape}\")\n",
    "        except Exception as e:\n",
    "            print(\"ERROR embedding\", filenames[i], e)\n",
    "            # append zeros to keep indices aligned\n",
    "            embeddings.append(np.zeros((model.hps['lowerdim'],), dtype='float32'))\n",
    "\n",
    "    embeddings = np.stack(embeddings, axis=0)\n",
    "    np.save(out_embeddings_path, embeddings)\n",
    "    # save mapping\n",
    "    id_map = {i: str(filenames[i]) for i in range(len(filenames))}\n",
    "    with open(out_map_path, \"w\") as f:\n",
    "        json.dump(id_map, f, indent=2)\n",
    "    print(\"[INFO] Saved embeddings:\", out_embeddings_path, \"shape:\", embeddings.shape)\n",
    "    print(\"[INFO] Saved mapping:\", out_map_path)\n",
    "    return embeddings, id_map\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "npz_path = \"./sketchformer_dataset/chunk_0.npz\"\n",
    "weights_dir = \"./sketch-transformer-tf2-cvpr_tform_cont/weights\"\n",
    "config_json = \"./sketch-transformer-tf2-cvpr_tform_cont/config.json\"  # optional\n",
    "model = build_model_and_restore(weights_dir, max_seq_len=200, config_json=config_json)\n",
    "\n",
    "extract_embeddings_from_npz(npz_path, model, out_embeddings_path=\"./sketchformer_embeddings.npy\", out_map_path=\"./id_mapping_sketchformer.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2b42bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded embeddings: (8, 128)\n",
      "âœ… Normalized embeddings for cosine similarity\n",
      "âœ… FAISS index created with 8 vectors\n",
      "ðŸ’¾ Saved FAISS index at: ./sketchformer_index.faiss\n",
      "ðŸ’¾ Saved ID mapping at: ./sketchformer_index_mapping.json\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Store SketchFormer embeddings into a FAISS index (cosine similarity).\n",
    "\n",
    "Steps:\n",
    "1. Load precomputed SketchFormer embeddings from .npy file.\n",
    "2. Normalize them for cosine similarity.\n",
    "3. Create a FAISS IndexFlatIP (Inner Product) index.\n",
    "4. Add embeddings and save index + mapping for later retrieval.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "def build_faiss_index_from_embeddings(\n",
    "        embeddings_path: str,\n",
    "        mapping_path: str,\n",
    "        index_output: str = \"sketchformer_index.faiss\",\n",
    "        normalized: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Build and save a FAISS index using cosine similarity.\n",
    "\n",
    "    ---\n",
    "    Parameters:\n",
    "        embeddings_path (str): Path to .npy file containing (N, D) embeddings.\n",
    "        mapping_path (str): Path to JSON or text file mapping IDs to filenames.\n",
    "        index_output (str): Output FAISS index file path.\n",
    "        normalized (bool): Whether to normalize embeddings before indexing.\n",
    "    ---\n",
    "    Returns:\n",
    "        (faiss.Index, dict): FAISS index and IDâ†’filename mapping.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load embeddings\n",
    "    embeddings = np.load(embeddings_path).astype('float32')\n",
    "    print(f\"âœ… Loaded embeddings: {embeddings.shape}\")\n",
    "\n",
    "    # Normalize embeddings for cosine similarity\n",
    "    if normalized:\n",
    "        norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "        norms[norms == 0] = 1e-10\n",
    "        embeddings = embeddings / norms\n",
    "        print(\"âœ… Normalized embeddings for cosine similarity\")\n",
    "\n",
    "    # Load mapping\n",
    "    if mapping_path.endswith(\".json\"):\n",
    "        with open(mapping_path, \"r\") as f:\n",
    "            id_mapping = json.load(f)\n",
    "    else:\n",
    "        with open(mapping_path, \"r\") as f:\n",
    "            filenames = [line.strip() for line in f.readlines()]\n",
    "        id_mapping = {i: filenames[i] for i in range(len(filenames))}\n",
    "\n",
    "    # Create FAISS index (Inner Product for cosine similarity)\n",
    "    dim = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatIP(dim)\n",
    "    index.add(embeddings)\n",
    "    print(f\"âœ… FAISS index created with {index.ntotal} vectors\")\n",
    "\n",
    "    # Save index and mapping\n",
    "    faiss.write_index(index, index_output)\n",
    "    print(f\"ðŸ’¾ Saved FAISS index at: {index_output}\")\n",
    "\n",
    "    json_output = os.path.splitext(index_output)[0] + \"_mapping.json\"\n",
    "    with open(json_output, \"w\") as f:\n",
    "        json.dump(id_mapping, f, indent=2)\n",
    "    print(f\"ðŸ’¾ Saved ID mapping at: {json_output}\")\n",
    "\n",
    "    return index, id_mapping\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "embeddings_path = \"./sketchformer_embeddings.npy\"\n",
    "mapping_path = \"./id_mapping_sketchformer.json\"   # adjust if you have another mapping\n",
    "index_output = \"./sketchformer_index.faiss\"\n",
    "\n",
    "index, id_mapping = build_faiss_index_from_embeddings(\n",
    "    embeddings_path=embeddings_path,\n",
    "    mapping_path=mapping_path,\n",
    "    index_output=index_output,\n",
    "    normalized=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f31b216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Restored checkpoint: ./sketch-transformer-tf2-cvpr_tform_cont/weights/ckpt-12\n",
      "\n",
      " Top similar sketches for pdf3_SIM.png: \n",
      "1. pdf7.png - score: 6.26%\n",
      "2. pdf2.png - score: 2.8%\n",
      "3. pdf4.png - score: 1.36%\n",
      "4. pdf8.png - score: 1.32%\n",
      "5. pdf3.png - score: 0.59%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "# Suppress TensorFlow and Keras logs\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "os.environ[\"TF_ENABLE_ONEDNN_OPTS\"] = \"0\"\n",
    "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"true\"   # prevents verbose GPU memory messages\n",
    "os.environ[\"TF_CPP_MIN_VLOG_LEVEL\"] = \"3\"          # suppress absl GPU device INFO\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "import tensorflow as tf\n",
    "import absl.logging\n",
    "absl.logging.set_verbosity(absl.logging.ERROR)\n",
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "from PIL import Image\n",
    "from pngs_to_continuous_strokes import img_path_to_seq\n",
    "from extract_sketchformer_embeddings import build_model_and_restore\n",
    "\n",
    "def extract_embedding_from_npz(npz: np.ndarray, model):\n",
    "    try:\n",
    "        out = model.encode_from_seq(inp_seq=npz)\n",
    "        # print(type(out))\n",
    "        emb = out['embedding']\n",
    "        if hasattr(emb, 'numpy'):\n",
    "            emb_np = emb.numpy()\n",
    "        else:\n",
    "            emb_np = np.array(emb)\n",
    "        if emb_np.ndim == 3:\n",
    "            emb_np = emb_np.mean(axis=1)\n",
    "        emb_np = emb_np.reshape(-1)\n",
    "        # print(f\"Embedding Generated: {emb_np.shape}\")\n",
    "        return emb_np.astype('float32')\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR Embedding:- {e}\")\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "    \n",
    "weights_dir = \"./sketch-transformer-tf2-cvpr_tform_cont/weights\"\n",
    "config_json = \"./sketch-transformer-tf2-cvpr_tform_cont/config.json\"\n",
    "faiss_index = \"./sketchformer_index.faiss\"\n",
    "mapping_path = \"./id_mapping_sketchformer.json\"\n",
    "top_k = 5\n",
    "query_folder = \"/home/ayushkum/archimera/query_png\"\n",
    "\n",
    "# Building Model using weights\n",
    "model = build_model_and_restore(weights_dir=weights_dir, max_seq_len=200, config_json=config_json)\n",
    "# print(\"Loaded lowerdim:\", model.hps['lowerdim'])\n",
    "\n",
    "for fort in os.listdir(query_folder):\n",
    "    if not fort.lower().endswith('.png'):\n",
    "        continue\n",
    "    query_path = os.path.join(query_folder, fort)\n",
    "    # print(query_path)\n",
    "    # Converting png to continuous stroke form\n",
    "    seq = img_path_to_seq(path=query_path, target_size=None, max_seq_len=200)\n",
    "    # print(f\"Sequence Shape before reshaping: {seq.shape}\")\n",
    "    # Adding batch dimension to make it compatible with SKetchFormer\n",
    "    seq = np.expand_dims(seq, axis=0)\n",
    "    # print(f\"Sequence Shape after reshape: {seq.shape}\")\n",
    "    \n",
    "\n",
    "    # Extracting Embeddings\n",
    "    embedding = extract_embedding_from_npz(npz=seq, model=model)\n",
    "    # Normalizing Embedding for similarity matching\n",
    "    norms = np.linalg.norm(embedding)\n",
    "    if norms == 0:\n",
    "        norms = 1e-10\n",
    "    emb_norm = embedding / norms\n",
    "    emb_norm = emb_norm.reshape(1, -1)\n",
    "    # Loading FAISS index and mapping\n",
    "    index = faiss.read_index(faiss_index)\n",
    "    with open(mapping_path, \"r\") as f:\n",
    "        id_mapping = json.load(f)\n",
    "    # Search top-k similar images\n",
    "    # print(\"Query embedding dim:\", emb_norm.shape[1])\n",
    "    # print(\"FAISS index dim:\", index.d)\n",
    "    D, I = index.search(emb_norm, top_k)\n",
    "\n",
    "    # Prepare results\n",
    "    results = []\n",
    "    for rank, (idx, dist) in enumerate(zip(I[0], D[0]), start=1):\n",
    "        fname = id_mapping.get(str(idx)) or id_mapping.get(idx)\n",
    "        score = dist\n",
    "        results.append(\n",
    "            {\n",
    "                \"rank\": rank,\n",
    "                \"filename\": fname,\n",
    "                \"score\": float(score),\n",
    "            }\n",
    "        )\n",
    "    print(f\"\\n Top similar sketches for {fort}: \")\n",
    "    for r in results:\n",
    "        print(f\"{r['rank']}. {r['filename']} - score: {round(r['score'] * 100, 2)}%\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a94c03",
   "metadata": {},
   "source": [
    "# Augmented Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb1b8f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Converted pdf1.png | Length: 200\n",
      "âœ… Converted pdf1_aug1.png | Length: 200\n",
      "âœ… Converted pdf1_aug10.png | Length: 200\n",
      "âœ… Converted pdf1_aug2.png | Length: 200\n",
      "âœ… Converted pdf1_aug3.png | Length: 200\n",
      "âœ… Converted pdf1_aug4.png | Length: 200\n",
      "âœ… Converted pdf1_aug5.png | Length: 200\n",
      "âœ… Converted pdf1_aug6.png | Length: 200\n",
      "âœ… Converted pdf1_aug7.png | Length: 200\n",
      "âœ… Converted pdf1_aug8.png | Length: 200\n",
      "âœ… Converted pdf1_aug9.png | Length: 200\n",
      "âœ… Converted pdf2.png | Length: 200\n",
      "âœ… Converted pdf2_aug1.png | Length: 200\n",
      "âœ… Converted pdf2_aug10.png | Length: 200\n",
      "âœ… Converted pdf2_aug2.png | Length: 200\n",
      "âœ… Converted pdf2_aug3.png | Length: 200\n",
      "âœ… Converted pdf2_aug4.png | Length: 200\n",
      "âœ… Converted pdf2_aug5.png | Length: 200\n",
      "âœ… Converted pdf2_aug6.png | Length: 200\n",
      "âœ… Converted pdf2_aug7.png | Length: 200\n",
      "âœ… Converted pdf2_aug8.png | Length: 200\n",
      "âœ… Converted pdf2_aug9.png | Length: 200\n",
      "âœ… Converted pdf3.png | Length: 200\n",
      "âœ… Converted pdf3_aug1.png | Length: 200\n",
      "âœ… Converted pdf3_aug10.png | Length: 200\n",
      "âœ… Converted pdf3_aug2.png | Length: 200\n",
      "âœ… Converted pdf3_aug3.png | Length: 200\n",
      "âœ… Converted pdf3_aug4.png | Length: 200\n",
      "âœ… Converted pdf3_aug5.png | Length: 200\n",
      "âœ… Converted pdf3_aug6.png | Length: 200\n",
      "âœ… Converted pdf3_aug7.png | Length: 200\n",
      "âœ… Converted pdf3_aug8.png | Length: 200\n",
      "âœ… Converted pdf3_aug9.png | Length: 200\n",
      "âœ… Converted pdf4.png | Length: 200\n",
      "âœ… Converted pdf4_aug1.png | Length: 200\n",
      "âœ… Converted pdf4_aug10.png | Length: 200\n",
      "âœ… Converted pdf4_aug2.png | Length: 200\n",
      "âœ… Converted pdf4_aug3.png | Length: 200\n",
      "âœ… Converted pdf4_aug4.png | Length: 200\n",
      "âœ… Converted pdf4_aug5.png | Length: 200\n",
      "âœ… Converted pdf4_aug6.png | Length: 200\n",
      "âœ… Converted pdf4_aug7.png | Length: 200\n",
      "âœ… Converted pdf4_aug8.png | Length: 200\n",
      "âœ… Converted pdf4_aug9.png | Length: 200\n",
      "âœ… Converted pdf5.png | Length: 200\n",
      "âœ… Converted pdf5_aug1.png | Length: 200\n",
      "âœ… Converted pdf5_aug10.png | Length: 200\n",
      "âœ… Converted pdf5_aug2.png | Length: 200\n",
      "âœ… Converted pdf5_aug3.png | Length: 200\n",
      "âœ… Converted pdf5_aug4.png | Length: 200\n",
      "âœ… Converted pdf5_aug5.png | Length: 200\n",
      "âœ… Converted pdf5_aug6.png | Length: 200\n",
      "âœ… Converted pdf5_aug7.png | Length: 200\n",
      "âœ… Converted pdf5_aug8.png | Length: 200\n",
      "âœ… Converted pdf5_aug9.png | Length: 200\n",
      "âœ… Converted pdf6.png | Length: 200\n",
      "âœ… Converted pdf6_aug1.png | Length: 200\n",
      "âœ… Converted pdf6_aug10.png | Length: 200\n",
      "âœ… Converted pdf6_aug2.png | Length: 200\n",
      "âœ… Converted pdf6_aug3.png | Length: 200\n",
      "âœ… Converted pdf6_aug4.png | Length: 200\n",
      "âœ… Converted pdf6_aug5.png | Length: 200\n",
      "âœ… Converted pdf6_aug6.png | Length: 200\n",
      "âœ… Converted pdf6_aug7.png | Length: 200\n",
      "âœ… Converted pdf6_aug8.png | Length: 200\n",
      "âœ… Converted pdf6_aug9.png | Length: 200\n",
      "âœ… Converted pdf7.png | Length: 200\n",
      "âœ… Converted pdf7_aug1.png | Length: 200\n",
      "âœ… Converted pdf7_aug10.png | Length: 200\n",
      "âœ… Converted pdf7_aug2.png | Length: 200\n",
      "âœ… Converted pdf7_aug3.png | Length: 200\n",
      "âœ… Converted pdf7_aug4.png | Length: 200\n",
      "âœ… Converted pdf7_aug5.png | Length: 200\n",
      "âœ… Converted pdf7_aug6.png | Length: 200\n",
      "âœ… Converted pdf7_aug7.png | Length: 200\n",
      "âœ… Converted pdf7_aug8.png | Length: 200\n",
      "âœ… Converted pdf7_aug9.png | Length: 200\n",
      "âœ… Converted pdf8.png | Length: 200\n",
      "âœ… Converted pdf8_aug1.png | Length: 200\n",
      "âœ… Converted pdf8_aug10.png | Length: 200\n",
      "âœ… Converted pdf8_aug2.png | Length: 200\n",
      "âœ… Converted pdf8_aug3.png | Length: 200\n",
      "âœ… Converted pdf8_aug4.png | Length: 200\n",
      "âœ… Converted pdf8_aug5.png | Length: 200\n",
      "âœ… Converted pdf8_aug6.png | Length: 200\n",
      "âœ… Converted pdf8_aug7.png | Length: 200\n",
      "âœ… Converted pdf8_aug8.png | Length: 200\n",
      "âœ… Converted pdf8_aug9.png | Length: 200\n",
      "\n",
      "ðŸ’¾ Saved dataset: /home/ayushkum/archimera/sketchformer/sketchformer_dataset/chunk_1.npz\n",
      "   Shape: (88, 200, 5)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# pngs_to_continuous_strokes.py\n",
    "\"\"\"\n",
    "Convert PNG sketch images into continuous stroke sequences expected by SketchFormer.\n",
    "Each stroke vector has 5 values: [dx, dy, pen_down, pen_up, pen_end/pad].\n",
    "Sequences longer than `max_seq_len` are adaptively reduced using average pooling\n",
    "(for continuous deltas) and voting-based aggregation (for binary pen states).\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from skimage.morphology import skeletonize\n",
    "from glob import glob\n",
    "\n",
    "# -------------------- Image Preprocessing --------------------\n",
    "def preprocess_image(path, target_size=None):\n",
    "    \"\"\"\n",
    "    Reads an image, binarizes, inverts (so stroke pixels=1), and skeletonizes.\n",
    "    \"\"\"\n",
    "    img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "    if img is None:\n",
    "        raise RuntimeError(f\"Could not read {path}\")\n",
    "    if target_size:\n",
    "        img = cv2.resize(img, target_size, interpolation=cv2.INTER_AREA)\n",
    "    # Adaptive threshold using Otsu\n",
    "    _, th = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    if np.mean(th) > 127:  # ensure black strokes on white bg\n",
    "        th = 255 - th\n",
    "    sk = skeletonize((th > 0))\n",
    "    return (sk.astype(np.uint8) * 255)\n",
    "\n",
    "\n",
    "# -------------------- Contour Extraction --------------------\n",
    "def image_to_polylines(img, min_length=5, approx_epsilon=2.0):\n",
    "    \"\"\"\n",
    "    Extracts skeleton contours and approximates them as polylines.\n",
    "    \"\"\"\n",
    "    contours, _ = cv2.findContours(img.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)\n",
    "    polylines = []\n",
    "    for cnt in contours:\n",
    "        if len(cnt) < min_length:\n",
    "            continue\n",
    "        cnt = cnt.squeeze()\n",
    "        if cnt.ndim != 2:\n",
    "            continue\n",
    "        approx = cv2.approxPolyDP(cnt.astype(np.int32), approx_epsilon, False)\n",
    "        approx = approx.squeeze()\n",
    "        if approx.ndim != 2:\n",
    "            continue\n",
    "        polylines.append(approx.astype(np.float32))\n",
    "    return polylines\n",
    "\n",
    "\n",
    "# -------------------- Polyline â†’ Sequence --------------------\n",
    "def polylines_to_continuous_seq(polylines):\n",
    "    \"\"\"\n",
    "    Convert polylines to [dx, dy, pen_down, pen_up, pen_end] sequence.\n",
    "    \"\"\"\n",
    "    seq = []\n",
    "    for i, poly in enumerate(polylines):\n",
    "        if poly.shape[0] < 2:\n",
    "            continue\n",
    "        deltas = np.diff(poly, axis=0)  # shape (n-1,2)\n",
    "        for dx, dy in deltas:\n",
    "            seq.append([float(dx), float(dy), 1.0, 0.0, 0.0])  # pen_down\n",
    "        # mark stroke end\n",
    "        if i < len(polylines) - 1:\n",
    "            seq.append([0.0, 0.0, 0.0, 1.0, 0.0])  # pen_up\n",
    "    if not seq:\n",
    "        seq = [[0.0, 0.0, 0.0, 0.0, 1.0]]  # fallback single token\n",
    "    return np.array(seq, dtype=np.float32)\n",
    "\n",
    "\n",
    "# -------------------- Sequence Pooling --------------------\n",
    "def pooled_pen_state(chunk, idx):\n",
    "    \"\"\"\n",
    "    Decide binary pen state (pen_down/up/end) for a pooled segment.\n",
    "    Uses hybrid logic: majority vote with fallback if any '1' present.\n",
    "    \"\"\"\n",
    "    avg = np.mean(chunk[:, idx])\n",
    "    if avg >= 0.5:\n",
    "        return 1.0\n",
    "    # elif np.any(chunk[:, idx] > 0.5):\n",
    "    #     return 1.0\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "def average_pool_sequence(seq, target_len):\n",
    "    \"\"\"\n",
    "    Average pool a (N,5) sequence to target_len for SketchFormer compatibility.\n",
    "    - dx, dy averaged normally\n",
    "    - pen_* flags aggregated via hybrid voting\n",
    "    \"\"\"\n",
    "    n = len(seq)\n",
    "    if n <= target_len:\n",
    "        return seq  # no pooling needed\n",
    "\n",
    "    step = n / target_len\n",
    "    pooled = []\n",
    "    for i in range(target_len):\n",
    "        start = int(i * step)\n",
    "        end = int((i + 1) * step)\n",
    "        chunk = seq[start:end]\n",
    "        if len(chunk) == 0:\n",
    "            continue\n",
    "        dx, dy = np.mean(chunk[:, 0]), np.mean(chunk[:, 1])\n",
    "        pen_down = pooled_pen_state(chunk, 2)\n",
    "        pen_up = pooled_pen_state(chunk, 3)\n",
    "        pen_end = pooled_pen_state(chunk, 4)\n",
    "        pooled.append([dx, dy, pen_down, pen_up, pen_end])\n",
    "\n",
    "    return np.array(pooled, dtype=np.float32)\n",
    "\n",
    "\n",
    "# -------------------- Sequence Padding --------------------\n",
    "def pad_sequence(seq, max_seq_len):\n",
    "    \"\"\"\n",
    "    Pad sequence to fixed length using [0,0,0,0,1].\n",
    "    \"\"\"\n",
    "    pad_token = [0.0, 0.0, 0.0, 0.0, 1.0]\n",
    "    if len(seq) > max_seq_len:\n",
    "        seq = seq[:max_seq_len]\n",
    "    elif len(seq) < max_seq_len:\n",
    "        pad = np.tile(pad_token, (max_seq_len - len(seq), 1))\n",
    "        seq = np.vstack([seq, pad])\n",
    "    return seq\n",
    "\n",
    "\n",
    "# -------------------- Conversion Pipeline --------------------\n",
    "def img_path_to_seq(path, target_size=None, max_seq_len=200):\n",
    "    \"\"\"\n",
    "    Complete conversion from PNG â†’ skeleton â†’ polylines â†’ sequence (pooled + padded).\n",
    "    \"\"\"\n",
    "    sk = preprocess_image(path, target_size=target_size)\n",
    "    polylines = image_to_polylines(sk, min_length=8, approx_epsilon=2.0)\n",
    "    seq = polylines_to_continuous_seq(polylines)\n",
    "\n",
    "    # adaptive pooling if too long\n",
    "    if len(seq) > max_seq_len:\n",
    "        seq = average_pool_sequence(seq, max_seq_len)\n",
    "\n",
    "    seq = pad_sequence(seq, max_seq_len)\n",
    "    return seq\n",
    "\n",
    "\n",
    "# -------------------- Main Routine --------------------\n",
    "def main(png_folder, out_npz, target_size=None, max_seq_len=200):\n",
    "    paths = sorted(glob(os.path.join(png_folder, '*.png')))\n",
    "    X, filenames = [], []\n",
    "\n",
    "    for p in paths:\n",
    "        try:\n",
    "            seq = img_path_to_seq(p, target_size=target_size, max_seq_len=max_seq_len)\n",
    "            X.append(seq)\n",
    "            filenames.append(os.path.basename(p))\n",
    "            print(f\"âœ… Converted {os.path.basename(p)} | Length: {np.sum(seq[..., -1] != 1)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Failed {p}: {e}\")\n",
    "\n",
    "    if not X:\n",
    "        raise RuntimeError(\"No sequences produced.\")\n",
    "\n",
    "    X = np.stack(X, axis=0)  # (N, max_seq_len, 5)\n",
    "    np.savez(out_npz, x=X, filenames=np.array(filenames))\n",
    "    print(f\"\\nðŸ’¾ Saved dataset: {out_npz}\")\n",
    "    print(f\"   Shape: {X.shape}\")\n",
    "\n",
    "\n",
    "# -------------------- Driver Code --------------------\n",
    "# if __name__ == \"__main__\":\n",
    "png_folder = \"/home/ayushkum/archimera/augmented/input_png\"\n",
    "out_npz = \"/home/ayushkum/archimera/sketchformer/sketchformer_dataset/chunk_1.npz\"\n",
    "main(png_folder=png_folder, out_npz=out_npz)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0626073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Restored checkpoint: ./sketch-transformer-tf2-cvpr_tform_cont/weights/ckpt-12\n",
      "[1/88] Embedded pdf1.png -> (128,)\n",
      "[2/88] Embedded pdf1_aug1.png -> (128,)\n",
      "[3/88] Embedded pdf1_aug10.png -> (128,)\n",
      "[4/88] Embedded pdf1_aug2.png -> (128,)\n",
      "[5/88] Embedded pdf1_aug3.png -> (128,)\n",
      "[6/88] Embedded pdf1_aug4.png -> (128,)\n",
      "[7/88] Embedded pdf1_aug5.png -> (128,)\n",
      "[8/88] Embedded pdf1_aug6.png -> (128,)\n",
      "[9/88] Embedded pdf1_aug7.png -> (128,)\n",
      "[10/88] Embedded pdf1_aug8.png -> (128,)\n",
      "[11/88] Embedded pdf1_aug9.png -> (128,)\n",
      "[12/88] Embedded pdf2.png -> (128,)\n",
      "[13/88] Embedded pdf2_aug1.png -> (128,)\n",
      "[14/88] Embedded pdf2_aug10.png -> (128,)\n",
      "[15/88] Embedded pdf2_aug2.png -> (128,)\n",
      "[16/88] Embedded pdf2_aug3.png -> (128,)\n",
      "[17/88] Embedded pdf2_aug4.png -> (128,)\n",
      "[18/88] Embedded pdf2_aug5.png -> (128,)\n",
      "[19/88] Embedded pdf2_aug6.png -> (128,)\n",
      "[20/88] Embedded pdf2_aug7.png -> (128,)\n",
      "[21/88] Embedded pdf2_aug8.png -> (128,)\n",
      "[22/88] Embedded pdf2_aug9.png -> (128,)\n",
      "[23/88] Embedded pdf3.png -> (128,)\n",
      "[24/88] Embedded pdf3_aug1.png -> (128,)\n",
      "[25/88] Embedded pdf3_aug10.png -> (128,)\n",
      "[26/88] Embedded pdf3_aug2.png -> (128,)\n",
      "[27/88] Embedded pdf3_aug3.png -> (128,)\n",
      "[28/88] Embedded pdf3_aug4.png -> (128,)\n",
      "[29/88] Embedded pdf3_aug5.png -> (128,)\n",
      "[30/88] Embedded pdf3_aug6.png -> (128,)\n",
      "[31/88] Embedded pdf3_aug7.png -> (128,)\n",
      "[32/88] Embedded pdf3_aug8.png -> (128,)\n",
      "[33/88] Embedded pdf3_aug9.png -> (128,)\n",
      "[34/88] Embedded pdf4.png -> (128,)\n",
      "[35/88] Embedded pdf4_aug1.png -> (128,)\n",
      "[36/88] Embedded pdf4_aug10.png -> (128,)\n",
      "[37/88] Embedded pdf4_aug2.png -> (128,)\n",
      "[38/88] Embedded pdf4_aug3.png -> (128,)\n",
      "[39/88] Embedded pdf4_aug4.png -> (128,)\n",
      "[40/88] Embedded pdf4_aug5.png -> (128,)\n",
      "[41/88] Embedded pdf4_aug6.png -> (128,)\n",
      "[42/88] Embedded pdf4_aug7.png -> (128,)\n",
      "[43/88] Embedded pdf4_aug8.png -> (128,)\n",
      "[44/88] Embedded pdf4_aug9.png -> (128,)\n",
      "[45/88] Embedded pdf5.png -> (128,)\n",
      "[46/88] Embedded pdf5_aug1.png -> (128,)\n",
      "[47/88] Embedded pdf5_aug10.png -> (128,)\n",
      "[48/88] Embedded pdf5_aug2.png -> (128,)\n",
      "[49/88] Embedded pdf5_aug3.png -> (128,)\n",
      "[50/88] Embedded pdf5_aug4.png -> (128,)\n",
      "[51/88] Embedded pdf5_aug5.png -> (128,)\n",
      "[52/88] Embedded pdf5_aug6.png -> (128,)\n",
      "[53/88] Embedded pdf5_aug7.png -> (128,)\n",
      "[54/88] Embedded pdf5_aug8.png -> (128,)\n",
      "[55/88] Embedded pdf5_aug9.png -> (128,)\n",
      "[56/88] Embedded pdf6.png -> (128,)\n",
      "[57/88] Embedded pdf6_aug1.png -> (128,)\n",
      "[58/88] Embedded pdf6_aug10.png -> (128,)\n",
      "[59/88] Embedded pdf6_aug2.png -> (128,)\n",
      "[60/88] Embedded pdf6_aug3.png -> (128,)\n",
      "[61/88] Embedded pdf6_aug4.png -> (128,)\n",
      "[62/88] Embedded pdf6_aug5.png -> (128,)\n",
      "[63/88] Embedded pdf6_aug6.png -> (128,)\n",
      "[64/88] Embedded pdf6_aug7.png -> (128,)\n",
      "[65/88] Embedded pdf6_aug8.png -> (128,)\n",
      "[66/88] Embedded pdf6_aug9.png -> (128,)\n",
      "[67/88] Embedded pdf7.png -> (128,)\n",
      "[68/88] Embedded pdf7_aug1.png -> (128,)\n",
      "[69/88] Embedded pdf7_aug10.png -> (128,)\n",
      "[70/88] Embedded pdf7_aug2.png -> (128,)\n",
      "[71/88] Embedded pdf7_aug3.png -> (128,)\n",
      "[72/88] Embedded pdf7_aug4.png -> (128,)\n",
      "[73/88] Embedded pdf7_aug5.png -> (128,)\n",
      "[74/88] Embedded pdf7_aug6.png -> (128,)\n",
      "[75/88] Embedded pdf7_aug7.png -> (128,)\n",
      "[76/88] Embedded pdf7_aug8.png -> (128,)\n",
      "[77/88] Embedded pdf7_aug9.png -> (128,)\n",
      "[78/88] Embedded pdf8.png -> (128,)\n",
      "[79/88] Embedded pdf8_aug1.png -> (128,)\n",
      "[80/88] Embedded pdf8_aug10.png -> (128,)\n",
      "[81/88] Embedded pdf8_aug2.png -> (128,)\n",
      "[82/88] Embedded pdf8_aug3.png -> (128,)\n",
      "[83/88] Embedded pdf8_aug4.png -> (128,)\n",
      "[84/88] Embedded pdf8_aug5.png -> (128,)\n",
      "[85/88] Embedded pdf8_aug6.png -> (128,)\n",
      "[86/88] Embedded pdf8_aug7.png -> (128,)\n",
      "[87/88] Embedded pdf8_aug8.png -> (128,)\n",
      "[88/88] Embedded pdf8_aug9.png -> (128,)\n",
      "[INFO] Saved embeddings: ./augmented_sketchformer_embeddings.npy shape: (88, 128)\n",
      "[INFO] Saved mapping: ./augmented_id_mapping_sketchformer.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[-0.61615586, -0.30742657,  0.60184425, ..., -0.96678734,\n",
       "          0.33750892, -1.0956132 ],\n",
       "        [-0.50324064, -0.01265862,  0.7727757 , ..., -1.1639427 ,\n",
       "          0.74915946, -1.1675646 ],\n",
       "        [-0.4512274 , -0.27525562,  0.4812553 , ..., -1.0523536 ,\n",
       "         -0.097603  , -1.0007298 ],\n",
       "        ...,\n",
       "        [-0.91827923, -0.5041115 ,  0.3071432 , ..., -1.1805677 ,\n",
       "          0.0538047 , -0.78230727],\n",
       "        [-1.2151366 , -0.31619388,  0.76675296, ..., -0.8838505 ,\n",
       "          0.47338352, -1.0330502 ],\n",
       "        [-1.1406724 , -0.64906555,  1.3115425 , ..., -0.973471  ,\n",
       "         -0.11435312, -1.229326  ]], shape=(88, 128), dtype=float32),\n",
       " {0: 'pdf1.png',\n",
       "  1: 'pdf1_aug1.png',\n",
       "  2: 'pdf1_aug10.png',\n",
       "  3: 'pdf1_aug2.png',\n",
       "  4: 'pdf1_aug3.png',\n",
       "  5: 'pdf1_aug4.png',\n",
       "  6: 'pdf1_aug5.png',\n",
       "  7: 'pdf1_aug6.png',\n",
       "  8: 'pdf1_aug7.png',\n",
       "  9: 'pdf1_aug8.png',\n",
       "  10: 'pdf1_aug9.png',\n",
       "  11: 'pdf2.png',\n",
       "  12: 'pdf2_aug1.png',\n",
       "  13: 'pdf2_aug10.png',\n",
       "  14: 'pdf2_aug2.png',\n",
       "  15: 'pdf2_aug3.png',\n",
       "  16: 'pdf2_aug4.png',\n",
       "  17: 'pdf2_aug5.png',\n",
       "  18: 'pdf2_aug6.png',\n",
       "  19: 'pdf2_aug7.png',\n",
       "  20: 'pdf2_aug8.png',\n",
       "  21: 'pdf2_aug9.png',\n",
       "  22: 'pdf3.png',\n",
       "  23: 'pdf3_aug1.png',\n",
       "  24: 'pdf3_aug10.png',\n",
       "  25: 'pdf3_aug2.png',\n",
       "  26: 'pdf3_aug3.png',\n",
       "  27: 'pdf3_aug4.png',\n",
       "  28: 'pdf3_aug5.png',\n",
       "  29: 'pdf3_aug6.png',\n",
       "  30: 'pdf3_aug7.png',\n",
       "  31: 'pdf3_aug8.png',\n",
       "  32: 'pdf3_aug9.png',\n",
       "  33: 'pdf4.png',\n",
       "  34: 'pdf4_aug1.png',\n",
       "  35: 'pdf4_aug10.png',\n",
       "  36: 'pdf4_aug2.png',\n",
       "  37: 'pdf4_aug3.png',\n",
       "  38: 'pdf4_aug4.png',\n",
       "  39: 'pdf4_aug5.png',\n",
       "  40: 'pdf4_aug6.png',\n",
       "  41: 'pdf4_aug7.png',\n",
       "  42: 'pdf4_aug8.png',\n",
       "  43: 'pdf4_aug9.png',\n",
       "  44: 'pdf5.png',\n",
       "  45: 'pdf5_aug1.png',\n",
       "  46: 'pdf5_aug10.png',\n",
       "  47: 'pdf5_aug2.png',\n",
       "  48: 'pdf5_aug3.png',\n",
       "  49: 'pdf5_aug4.png',\n",
       "  50: 'pdf5_aug5.png',\n",
       "  51: 'pdf5_aug6.png',\n",
       "  52: 'pdf5_aug7.png',\n",
       "  53: 'pdf5_aug8.png',\n",
       "  54: 'pdf5_aug9.png',\n",
       "  55: 'pdf6.png',\n",
       "  56: 'pdf6_aug1.png',\n",
       "  57: 'pdf6_aug10.png',\n",
       "  58: 'pdf6_aug2.png',\n",
       "  59: 'pdf6_aug3.png',\n",
       "  60: 'pdf6_aug4.png',\n",
       "  61: 'pdf6_aug5.png',\n",
       "  62: 'pdf6_aug6.png',\n",
       "  63: 'pdf6_aug7.png',\n",
       "  64: 'pdf6_aug8.png',\n",
       "  65: 'pdf6_aug9.png',\n",
       "  66: 'pdf7.png',\n",
       "  67: 'pdf7_aug1.png',\n",
       "  68: 'pdf7_aug10.png',\n",
       "  69: 'pdf7_aug2.png',\n",
       "  70: 'pdf7_aug3.png',\n",
       "  71: 'pdf7_aug4.png',\n",
       "  72: 'pdf7_aug5.png',\n",
       "  73: 'pdf7_aug6.png',\n",
       "  74: 'pdf7_aug7.png',\n",
       "  75: 'pdf7_aug8.png',\n",
       "  76: 'pdf7_aug9.png',\n",
       "  77: 'pdf8.png',\n",
       "  78: 'pdf8_aug1.png',\n",
       "  79: 'pdf8_aug10.png',\n",
       "  80: 'pdf8_aug2.png',\n",
       "  81: 'pdf8_aug3.png',\n",
       "  82: 'pdf8_aug4.png',\n",
       "  83: 'pdf8_aug5.png',\n",
       "  84: 'pdf8_aug6.png',\n",
       "  85: 'pdf8_aug7.png',\n",
       "  86: 'pdf8_aug8.png',\n",
       "  87: 'pdf8_aug9.png'})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# extract_sketchformer_embeddings.py\n",
    "\"\"\"\n",
    "Load SketchFormer model from sketchformer-master, restore checkpoint, and extract embeddings.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "sys.path.append(\"./sketchformer_master\")\n",
    "\n",
    "# make sure repo is on path\n",
    "from sketchformer_master.models.sketchformer import Transformer as SketchTransformer\n",
    "from sketchformer_master.utils.hparams import HParams\n",
    "\n",
    "class DummyDataset:\n",
    "    \"\"\"Minimal dataset shim required by model __init__\"\"\"\n",
    "    def __init__(self, max_seq_len=200):\n",
    "        self.hps = {'use_continuous_data': True, 'max_seq_len': max_seq_len}\n",
    "        self.n_samples = 1\n",
    "        self.num_classes = 1\n",
    "        self.n_classes = 1\n",
    "        self.tokenizer = None\n",
    "\n",
    "def load_hparams_from_config(config_path=None):\n",
    "    \"\"\"\n",
    "    Create hparams from default then override with config.json if present.\n",
    "    \"\"\"\n",
    "    hps = SketchTransformer.specific_default_hparams()\n",
    "    if config_path and os.path.exists(config_path):\n",
    "        import json\n",
    "        cfg = json.load(open(config_path))\n",
    "        for k, v in cfg.items():\n",
    "            # only update keys that exist; HParams supports item assignment\n",
    "            try:\n",
    "                hps[k] = v\n",
    "            except Exception:\n",
    "                # ignore unknown keys\n",
    "                pass\n",
    "    return hps\n",
    "\n",
    "def build_model_and_restore(weights_dir, max_seq_len=200, config_json=None):\n",
    "    dataset = DummyDataset(max_seq_len=max_seq_len)\n",
    "    hps = load_hparams_from_config(config_json)\n",
    "\n",
    "    \n",
    "    # instantiate model\n",
    "    model = SketchTransformer(hps, dataset, out_dir='.', experiment_id='inference')\n",
    "    # build model graph: call build_model to create layers\n",
    "    model.build_model()\n",
    "    # restore checkpoint\n",
    "    ckpt = tf.train.Checkpoint(model=model)\n",
    "    latest = tf.train.latest_checkpoint(weights_dir)\n",
    "    if latest is None:\n",
    "        raise RuntimeError(f\"No checkpoint found in {weights_dir}\")\n",
    "    ckpt.restore(latest).expect_partial()\n",
    "    print(\"[INFO] Restored checkpoint:\", latest)\n",
    "    return model\n",
    "\n",
    "def extract_embeddings_from_npz(npz_path, model, out_embeddings_path=\"sketchformer_embeddings.npy\", out_map_path=\"id_mapping.json\"):\n",
    "    d = np.load(npz_path, allow_pickle=True)\n",
    "    X = d['x']  # shape (N, seq_len, 5)\n",
    "    filenames = d.get('filenames', None)\n",
    "    if filenames is None:\n",
    "        # try fallback\n",
    "        filenames = [f\"img_{i}.png\" for i in range(X.shape[0])]\n",
    "    embeddings = []\n",
    "    for i in range(X.shape[0]):\n",
    "        seq = X[i]  # (seq_len, 5)\n",
    "        # model.encode_from_seq expects a 1D or 2D array representing the sequence (it casts to np.array)\n",
    "        seq = np.expand_dims(seq, axis=0)\n",
    "        try:\n",
    "            out = model.encode_from_seq(seq)\n",
    "            emb = out['embedding']  # should be (1, lowerdim) or (1, seq_len, ...)\n",
    "            # ensure emb is numpy\n",
    "            if hasattr(emb, 'numpy'):\n",
    "                emb_np = emb.numpy()\n",
    "            else:\n",
    "                emb_np = np.array(emb)\n",
    "            # if emb has shape (1, d) or (1, seq_len, d), reduce to (d,)\n",
    "            if emb_np.ndim == 3:\n",
    "                # average across sequence axis if needed\n",
    "                emb_np = emb_np.mean(axis=1)\n",
    "            emb_np = emb_np.reshape(-1)\n",
    "            embeddings.append(emb_np.astype('float32'))\n",
    "            print(f\"[{i+1}/{X.shape[0]}] Embedded {filenames[i]} -> {emb_np.shape}\")\n",
    "        except Exception as e:\n",
    "            print(\"ERROR embedding\", filenames[i], e)\n",
    "            # append zeros to keep indices aligned\n",
    "            embeddings.append(np.zeros((model.hps['lowerdim'],), dtype='float32'))\n",
    "\n",
    "    embeddings = np.stack(embeddings, axis=0)\n",
    "    np.save(out_embeddings_path, embeddings)\n",
    "    # save mapping\n",
    "    id_map = {i: str(filenames[i]) for i in range(len(filenames))}\n",
    "    with open(out_map_path, \"w\") as f:\n",
    "        json.dump(id_map, f, indent=2)\n",
    "    print(\"[INFO] Saved embeddings:\", out_embeddings_path, \"shape:\", embeddings.shape)\n",
    "    print(\"[INFO] Saved mapping:\", out_map_path)\n",
    "    return embeddings, id_map\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "npz_path = \"./sketchformer_dataset/chunk_1.npz\"\n",
    "weights_dir = \"./sketch-transformer-tf2-cvpr_tform_cont/weights\"\n",
    "config_json = \"./sketch-transformer-tf2-cvpr_tform_cont/config.json\"  # optional\n",
    "model = build_model_and_restore(weights_dir, max_seq_len=200, config_json=config_json)\n",
    "\n",
    "extract_embeddings_from_npz(npz_path, model, out_embeddings_path=\"./augmented_sketchformer_embeddings.npy\", out_map_path=\"./augmented_id_mapping_sketchformer.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "decbe861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded embeddings: (88, 128)\n",
      "âœ… Normalized embeddings for cosine similarity\n",
      "âœ… FAISS index created with 88 vectors\n",
      "ðŸ’¾ Saved FAISS index at: ./augmented_sketchformer_index.faiss\n",
      "ðŸ’¾ Saved ID mapping at: ./augmented_sketchformer_index_mapping.json\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Store SketchFormer embeddings into a FAISS index (cosine similarity).\n",
    "\n",
    "Steps:\n",
    "1. Load precomputed SketchFormer embeddings from .npy file.\n",
    "2. Normalize them for cosine similarity.\n",
    "3. Create a FAISS IndexFlatIP (Inner Product) index.\n",
    "4. Add embeddings and save index + mapping for later retrieval.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "def build_faiss_index_from_embeddings(\n",
    "        embeddings_path: str,\n",
    "        mapping_path: str,\n",
    "        index_output: str = \"sketchformer_index.faiss\",\n",
    "        normalized: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Build and save a FAISS index using cosine similarity.\n",
    "\n",
    "    ---\n",
    "    Parameters:\n",
    "        embeddings_path (str): Path to .npy file containing (N, D) embeddings.\n",
    "        mapping_path (str): Path to JSON or text file mapping IDs to filenames.\n",
    "        index_output (str): Output FAISS index file path.\n",
    "        normalized (bool): Whether to normalize embeddings before indexing.\n",
    "    ---\n",
    "    Returns:\n",
    "        (faiss.Index, dict): FAISS index and IDâ†’filename mapping.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load embeddings\n",
    "    embeddings = np.load(embeddings_path).astype('float32')\n",
    "    print(f\"âœ… Loaded embeddings: {embeddings.shape}\")\n",
    "\n",
    "    # Normalize embeddings for cosine similarity\n",
    "    if normalized:\n",
    "        norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "        norms[norms == 0] = 1e-10\n",
    "        embeddings = embeddings / norms\n",
    "        print(\"âœ… Normalized embeddings for cosine similarity\")\n",
    "\n",
    "    # Load mapping\n",
    "    if mapping_path.endswith(\".json\"):\n",
    "        with open(mapping_path, \"r\") as f:\n",
    "            id_mapping = json.load(f)\n",
    "    else:\n",
    "        with open(mapping_path, \"r\") as f:\n",
    "            filenames = [line.strip() for line in f.readlines()]\n",
    "        id_mapping = {i: filenames[i] for i in range(len(filenames))}\n",
    "\n",
    "    # Create FAISS index (Inner Product for cosine similarity)\n",
    "    dim = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatIP(dim)\n",
    "    index.add(embeddings)\n",
    "    print(f\"âœ… FAISS index created with {index.ntotal} vectors\")\n",
    "\n",
    "    # Save index and mapping\n",
    "    faiss.write_index(index, index_output)\n",
    "    print(f\"ðŸ’¾ Saved FAISS index at: {index_output}\")\n",
    "\n",
    "    json_output = os.path.splitext(index_output)[0] + \"_mapping.json\"\n",
    "    with open(json_output, \"w\") as f:\n",
    "        json.dump(id_mapping, f, indent=2)\n",
    "    print(f\"ðŸ’¾ Saved ID mapping at: {json_output}\")\n",
    "\n",
    "    return index, id_mapping\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "embeddings_path = \"./augmented_sketchformer_embeddings.npy\"\n",
    "mapping_path = \"./augmented_id_mapping_sketchformer.json\"   # adjust if you have another mapping\n",
    "index_output = \"./augmented_sketchformer_index.faiss\"\n",
    "\n",
    "index, id_mapping = build_faiss_index_from_embeddings(\n",
    "    embeddings_path=embeddings_path,\n",
    "    mapping_path=mapping_path,\n",
    "    index_output=index_output,\n",
    "    normalized=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ebfaeb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Restored checkpoint: ./sketch-transformer-tf2-cvpr_tform_cont/weights/ckpt-12\n",
      "\n",
      " Top similar sketches for pdf3_SIM.png: \n",
      "1. pdf8_aug10.png - score: 18.62%\n",
      "2. pdf8_aug4.png - score: 16.46%\n",
      "3. pdf8_aug1.png - score: 16.27%\n",
      "4. pdf8.png - score: 16.27%\n",
      "5. pdf8_aug8.png - score: 16.05%\n",
      "6. pdf8_aug3.png - score: 15.37%\n",
      "7. pdf8_aug9.png - score: 13.21%\n",
      "8. pdf8_aug2.png - score: 12.87%\n",
      "9. pdf7.png - score: 12.77%\n",
      "10. pdf1_aug1.png - score: 12.74%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "# Suppress TensorFlow and Keras logs\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "os.environ[\"TF_ENABLE_ONEDNN_OPTS\"] = \"0\"\n",
    "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"true\"   # prevents verbose GPU memory messages\n",
    "os.environ[\"TF_CPP_MIN_VLOG_LEVEL\"] = \"3\"          # suppress absl GPU device INFO\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "import tensorflow as tf\n",
    "import absl.logging\n",
    "absl.logging.set_verbosity(absl.logging.ERROR)\n",
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "from PIL import Image\n",
    "from pngs_to_continuous_strokes import img_path_to_seq\n",
    "from extract_sketchformer_embeddings import build_model_and_restore\n",
    "\n",
    "def extract_embedding_from_npz(npz: np.ndarray, model):\n",
    "    try:\n",
    "        out = model.encode_from_seq(inp_seq=npz)\n",
    "        # print(type(out))\n",
    "        emb = out['embedding']\n",
    "        if hasattr(emb, 'numpy'):\n",
    "            emb_np = emb.numpy()\n",
    "        else:\n",
    "            emb_np = np.array(emb)\n",
    "        if emb_np.ndim == 3:\n",
    "            emb_np = emb_np.mean(axis=1)\n",
    "        emb_np = emb_np.reshape(-1)\n",
    "        # print(f\"Embedding Generated: {emb_np.shape}\")\n",
    "        return emb_np.astype('float32')\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR Embedding:- {e}\")\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "    \n",
    "weights_dir = \"./sketch-transformer-tf2-cvpr_tform_cont/weights\"\n",
    "config_json = \"./sketch-transformer-tf2-cvpr_tform_cont/config.json\"\n",
    "faiss_index = \"./augmented_sketchformer_index.faiss\"\n",
    "mapping_path = \"./augmented_id_mapping_sketchformer.json\"\n",
    "top_k = 10\n",
    "query_folder = \"/home/ayushkum/archimera/query_png\"\n",
    "\n",
    "# Building Model using weights\n",
    "model = build_model_and_restore(weights_dir=weights_dir, max_seq_len=200, config_json=config_json)\n",
    "# print(\"Loaded lowerdim:\", model.hps['lowerdim'])\n",
    "\n",
    "for fort in os.listdir(query_folder):\n",
    "    if not fort.lower().endswith('.png'):\n",
    "        continue\n",
    "    query_path = os.path.join(query_folder, fort)\n",
    "    # print(query_path)\n",
    "    # Converting png to continuous stroke form\n",
    "    seq = img_path_to_seq(path=query_path, target_size=None, max_seq_len=200)\n",
    "    # print(f\"Sequence Shape before reshaping: {seq.shape}\")\n",
    "    # Adding batch dimension to make it compatible with SKetchFormer\n",
    "    seq = np.expand_dims(seq, axis=0)\n",
    "    # print(f\"Sequence Shape after reshape: {seq.shape}\")\n",
    "    \n",
    "\n",
    "    # Extracting Embeddings\n",
    "    embedding = extract_embedding_from_npz(npz=seq, model=model)\n",
    "    # Normalizing Embedding for similarity matching\n",
    "    norms = np.linalg.norm(embedding)\n",
    "    if norms == 0:\n",
    "        norms = 1e-10\n",
    "    emb_norm = embedding / norms\n",
    "    emb_norm = emb_norm.reshape(1, -1)\n",
    "    # Loading FAISS index and mapping\n",
    "    index = faiss.read_index(faiss_index)\n",
    "    with open(mapping_path, \"r\") as f:\n",
    "        id_mapping = json.load(f)\n",
    "    # Search top-k similar images\n",
    "    # print(\"Query embedding dim:\", emb_norm.shape[1])\n",
    "    # print(\"FAISS index dim:\", index.d)\n",
    "    D, I = index.search(emb_norm, top_k)\n",
    "\n",
    "    # Prepare results\n",
    "    results = []\n",
    "    for rank, (idx, dist) in enumerate(zip(I[0], D[0]), start=1):\n",
    "        fname = id_mapping.get(str(idx)) or id_mapping.get(idx)\n",
    "        score = dist\n",
    "        results.append(\n",
    "            {\n",
    "                \"rank\": rank,\n",
    "                \"filename\": fname,\n",
    "                \"score\": float(score),\n",
    "            }\n",
    "        )\n",
    "    print(f\"\\n Top similar sketches for {fort}: \")\n",
    "    for r in results:\n",
    "        print(f\"{r['rank']}. {r['filename']} - score: {round(r['score'] * 100, 2)}%\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966a0641",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
