{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c6a7f5c",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook will contain a workflow for `ViT-g-14` with pretrained weights `laion2B-s12B-b42k`. This workflow will also include tiling to capture entire image as the model has a restriction of (1024, 1024, 3)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664eb2da",
   "metadata": {},
   "source": [
    "## Embedding Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47b84d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1560/1918890062.py:90: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.no_grad(), torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed pdf5.png (63 tiles)\n",
      "Processed pdf8.png (63 tiles)\n",
      "Processed pdf4.png (63 tiles)\n",
      "Processed pdf3.png (63 tiles)\n",
      "Processed pdf1.png (63 tiles)\n",
      "Processed pdf6.png (63 tiles)\n",
      "Processed pdf7.png (63 tiles)\n",
      "Processed pdf2.png (63 tiles)\n",
      "Total embeddings computed: (8, 1024)\n",
      "FAISS index saved at: /home/ayushkum/archimera/vit-g-14/sketch_index.faiss\n",
      "Mapping saved at: /home/ayushkum/archimera/vit-g-14/id_mapping.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import faiss\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import open_clip\n",
    "\n",
    "# ===================== #\n",
    "#   Image Preprocessing #\n",
    "# ===================== #\n",
    "def preprocess_sketch(img_path):\n",
    "    \"\"\"Enhance edges, clean noise, and produce normalized binary image.\"\"\"\n",
    "    img = cv2.imread(img_path)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    blur = cv2.bilateralFilter(gray, 9, 75, 75)\n",
    "    norm = cv2.normalize(blur, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    edges = cv2.Canny(norm, 50, 150)\n",
    "    _, binary = cv2.threshold(edges, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    kernel = np.ones((3, 3), np.uint8)\n",
    "    refined = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)\n",
    "    processed = cv2.bitwise_not(refined)\n",
    "    processed_rgb = cv2.cvtColor(processed, cv2.COLOR_GRAY2RGB)\n",
    "    return processed_rgb\n",
    "\n",
    "# def preprocess_sketch(img_path):\n",
    "#     \"\"\"\n",
    "#     Dummy preprocessing function.\n",
    "#     (Placeholder to maintain workflow compatibility ‚Äî returns RGB image as-is.)\n",
    "#     \"\"\"\n",
    "#     img = cv2.imread(img_path)\n",
    "#     if img is None:\n",
    "#         raise ValueError(f\"Could not read image: {img_path}\")\n",
    "#     img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "#     return img_rgb\n",
    "\n",
    "# ===================== #\n",
    "#   Tiling for CLIP     #\n",
    "# ===================== #\n",
    "def tile_image(image, tile_size=1024, overlap=0.2):\n",
    "    \"\"\"Split image into overlapping tiles for high-resolution processing.\"\"\"\n",
    "    w, h = image.size\n",
    "    step = int(tile_size * (1 - overlap))\n",
    "    tiles = []\n",
    "    for y in range(0, h, step):\n",
    "        for x in range(0, w, step):\n",
    "            box = (x, y, min(x + tile_size, w), min(y + tile_size, h))\n",
    "            tile = image.crop(box)\n",
    "            tiles.append(tile)\n",
    "    return tiles\n",
    "\n",
    "# ===================== #\n",
    "#   Build FAISS Index   #\n",
    "# ===================== #\n",
    "def build_faiss_index(\n",
    "    image_folder: str,\n",
    "    index_path: str = \"./sketch_index.faiss\",\n",
    "    mapping_path: str = \"./id_mapping.json\",\n",
    "    model_name: str = \"ViT-g-14\",\n",
    "    pretrained: str = \"laion2B-s12B-b42K\",\n",
    "    distance_metric: str = \"cosine\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Build FAISS index from large sketch images using LAION CLIP (ViT-g-14).\n",
    "    Automatically tiles and preprocesses large images before embedding.\n",
    "    \"\"\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    model, _, preprocess = open_clip.create_model_and_transforms(model_name, pretrained=pretrained)\n",
    "    model = model.to(device)\n",
    "    tokenizer = open_clip.get_tokenizer(model_name)\n",
    "\n",
    "    embeddings, filenames = [], []\n",
    "\n",
    "    for fname in os.listdir(image_folder):\n",
    "        if fname.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
    "            path = os.path.join(image_folder, fname)\n",
    "            try:\n",
    "                pre_img = preprocess_sketch(path)\n",
    "                pil_img = Image.fromarray(pre_img)\n",
    "\n",
    "                # --- Tile Large Image ---\n",
    "                tiles = tile_image(pil_img, tile_size=1024, overlap=0.2)\n",
    "                tile_embeds = []\n",
    "\n",
    "                for tile in tiles:\n",
    "                    tile_input = preprocess(tile).unsqueeze(0).to(device)\n",
    "                    with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "                        emb = model.encode_image(tile_input)\n",
    "                    emb = emb / emb.norm(dim=-1, keepdim=True)\n",
    "                    tile_embeds.append(emb.cpu().numpy())\n",
    "\n",
    "                # --- Pool Tile Embeddings ---\n",
    "                image_embed = np.mean(np.vstack(tile_embeds), axis=0)\n",
    "                embeddings.append(image_embed.astype(\"float32\"))\n",
    "                filenames.append(fname)\n",
    "\n",
    "                print(f\"Processed {fname} ({len(tiles)} tiles)\")\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping {fname}: {e}\")\n",
    "\n",
    "    if not embeddings:\n",
    "        raise ValueError(f\"No valid images found in {image_folder}\")\n",
    "\n",
    "    embeddings = np.stack(embeddings, axis=0)\n",
    "    print(f\"Total embeddings computed: {embeddings.shape}\")\n",
    "\n",
    "    # --- Build FAISS Index ---\n",
    "    dim = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatIP(dim) if distance_metric == \"cosine\" else faiss.IndexFlatL2(dim)\n",
    "    index.add(embeddings)\n",
    "\n",
    "    faiss.write_index(index, index_path)\n",
    "    print(f\"FAISS index saved at: {index_path}\")\n",
    "\n",
    "    id_mapping = {i: filenames[i] for i in range(len(filenames))}\n",
    "    with open(mapping_path, \"w\") as f:\n",
    "        json.dump(id_mapping, f, indent=2)\n",
    "    print(f\"Mapping saved at: {mapping_path}\")\n",
    "\n",
    "    return index, id_mapping\n",
    "\n",
    "# ===================== #\n",
    "#       Example Run     #\n",
    "# ===================== #\n",
    "image_folder = \"/home/ayushkum/archimera/inputs/input_png\"\n",
    "index_path = \"/home/ayushkum/archimera/vit-g-14/sketch_index.faiss\"\n",
    "mapping_path = \"/home/ayushkum/archimera/vit-g-14/id_mapping.json\"\n",
    "\n",
    "index, mapping = build_faiss_index(\n",
    "    image_folder=image_folder,\n",
    "    index_path=index_path,\n",
    "    mapping_path=mapping_path,\n",
    "    model_name=\"ViT-g-14\",\n",
    "    pretrained=\"/home/ayushkum/archimera/vit-g-14/open_clip_pytorch_model.bin\",\n",
    "    distance_metric=\"cosine\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e7a987",
   "metadata": {},
   "source": [
    "## Query Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a69932ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Query: pdf3_SIM.png\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1560/2057992544.py:105: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.no_grad(), torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. pdf2.png ‚Äî score: 86.01%\n",
      "2. pdf8.png ‚Äî score: 85.17%\n",
      "3. pdf4.png ‚Äî score: 84.83%\n",
      "4. pdf7.png ‚Äî score: 84.48%\n",
      "5. pdf3.png ‚Äî score: 83.77%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import faiss\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import open_clip\n",
    "\n",
    "\n",
    "# ===================== #\n",
    "#   Image Preprocessing #\n",
    "# ===================== #\n",
    "def preprocess_sketch(img_path):\n",
    "    \"\"\"Enhance edges, clean noise, and produce normalized binary image.\"\"\"\n",
    "    img = cv2.imread(img_path)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    blur = cv2.bilateralFilter(gray, 9, 75, 75)\n",
    "    norm = cv2.normalize(blur, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    edges = cv2.Canny(norm, 50, 150)\n",
    "    _, binary = cv2.threshold(edges, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    kernel = np.ones((3, 3), np.uint8)\n",
    "    refined = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)\n",
    "    processed = cv2.bitwise_not(refined)\n",
    "    processed_rgb = cv2.cvtColor(processed, cv2.COLOR_GRAY2RGB)\n",
    "    return processed_rgb\n",
    "\n",
    "# def preprocess_sketch(img_path):\n",
    "#     \"\"\"\n",
    "#     Dummy preprocessing function.\n",
    "#     (Placeholder to maintain workflow compatibility ‚Äî returns RGB image as-is.)\n",
    "#     \"\"\"\n",
    "#     img = cv2.imread(img_path)\n",
    "#     if img is None:\n",
    "#         raise ValueError(f\"Could not read image: {img_path}\")\n",
    "#     img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "#     return img_rgb\n",
    "\n",
    "# ===================== #\n",
    "#   Tiling for CLIP     #\n",
    "# ===================== #\n",
    "def tile_image(image, tile_size=1024, overlap=0.2):\n",
    "    \"\"\"Split large query image into overlapping tiles.\"\"\"\n",
    "    w, h = image.size\n",
    "    step = int(tile_size * (1 - overlap))\n",
    "    tiles = []\n",
    "    for y in range(0, h, step):\n",
    "        for x in range(0, w, step):\n",
    "            box = (x, y, min(x + tile_size, w), min(y + tile_size, h))\n",
    "            tile = image.crop(box)\n",
    "            tiles.append(tile)\n",
    "    return tiles\n",
    "\n",
    "\n",
    "# ===================== #\n",
    "#   Search Function     #\n",
    "# ===================== #\n",
    "def search_similar_sketches(\n",
    "    query_path: str,\n",
    "    index_path: str = \"./sketch_index.faiss\",\n",
    "    mapping_path: str = \"./id_mapping.json\",\n",
    "    model_name: str = \"ViT-g-14\",\n",
    "    pretrained: str = \"laion2B-s12B-b42K\",\n",
    "    top_k: int = 5,\n",
    "    distance_metric: str = \"cosine\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Search for similar sketches using LAION CLIP embeddings and FAISS index.\n",
    "\n",
    "    ---\n",
    "    Parameters:\n",
    "        query_path (str): Path to query image (large architectural sketch).\n",
    "        index_path (str): Path to FAISS index file.\n",
    "        mapping_path (str): Path to JSON mapping (ID -> filename).\n",
    "        model_name (str): OpenCLIP model name (default ViT-g-14).\n",
    "        pretrained (str): Pretrained weights for OpenCLIP.\n",
    "        top_k (int): Number of results to return.\n",
    "        distance_metric (str): 'cosine' or 'L2'.\n",
    "\n",
    "    ---\n",
    "    Returns:\n",
    "        list[dict]: ranked list of matches with filenames and scores.\n",
    "    \"\"\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Load FAISS index and mapping\n",
    "    index = faiss.read_index(index_path)\n",
    "    with open(mapping_path, \"r\") as f:\n",
    "        id_mapping = json.load(f)\n",
    "\n",
    "    # Load OpenCLIP model\n",
    "    model, _, preprocess = open_clip.create_model_and_transforms(model_name, pretrained=pretrained)\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Preprocess + Tile Query Image\n",
    "    pre_img = preprocess_sketch(query_path)\n",
    "    pil_img = Image.fromarray(pre_img)\n",
    "    tiles = tile_image(pil_img, tile_size=1024, overlap=0.2)\n",
    "\n",
    "    tile_embeds = []\n",
    "    for tile in tiles:\n",
    "        tile_input = preprocess(tile).unsqueeze(0).to(device)\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "            emb = model.encode_image(tile_input)\n",
    "        emb = emb / emb.norm(dim=-1, keepdim=True)\n",
    "        tile_embeds.append(emb.cpu().numpy())\n",
    "\n",
    "    # Mean-pool tile embeddings ‚Üí one vector for whole query image\n",
    "    query_emb = np.mean(np.vstack(tile_embeds), axis=0).astype(\"float32\")\n",
    "    query_emb = np.expand_dims(query_emb, axis=0)\n",
    "\n",
    "    # Search\n",
    "    D, I = index.search(query_emb, top_k)\n",
    "\n",
    "    # Prepare results\n",
    "    results = []\n",
    "    for rank, (idx, dist) in enumerate(zip(I[0], D[0]), start=1):\n",
    "        fname = id_mapping.get(str(idx)) or id_mapping.get(idx)\n",
    "        score = dist if distance_metric.lower() == \"cosine\" else (1 / (1 + dist))\n",
    "        results.append({\n",
    "            \"rank\": rank,\n",
    "            \"filename\": fname,\n",
    "            \"score\": float(score),\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# ===================== #\n",
    "#     Example Run       #\n",
    "# ===================== #\n",
    "if __name__ == \"__main__\":\n",
    "    query_folder = \"/home/ayushkum/archimera/query_png\"\n",
    "    index_path = \"/home/ayushkum/archimera/vit-g-14/sketch_index.faiss\"\n",
    "    mapping_path = \"/home/ayushkum/archimera/vit-g-14/id_mapping.json\"\n",
    "\n",
    "    top_k = 5\n",
    "    distance_metric = \"cosine\"\n",
    "\n",
    "    for filename in os.listdir(query_folder):\n",
    "        if filename.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
    "            query_image_path = os.path.join(query_folder, filename)\n",
    "            print(f\"\\nüîç Query: {filename}\")\n",
    "            results = search_similar_sketches(\n",
    "                query_path=query_image_path,\n",
    "                index_path=index_path,\n",
    "                mapping_path=mapping_path,\n",
    "                top_k=top_k,\n",
    "                distance_metric=distance_metric,\n",
    "                model_name=\"ViT-g-14\",\n",
    "                pretrained=\"/home/ayushkum/archimera/vit-g-14/open_clip_pytorch_model.bin\"\n",
    "            )\n",
    "\n",
    "            for r in results:\n",
    "                print(f\"{r['rank']}. {r['filename']} ‚Äî score: {round(r['score'] * 100, 2)}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b101d9e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
